{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoCo Self-Supervised Learning (pretraining)\n",
    "\n",
    "***backbone***: Cifar-10\n",
    "\n",
    "***using simulated multiple GPUs parallel computing in only one GPU***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "CvAFrPlS4bLU",
    "outputId": "f4ca04c1-67b5-4938-b6d0-dc414ab4cdd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 26 02:18:05 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:18:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    44W / 300W |      0MiB / 16160MiB |      0%   E. Process |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi -i 0\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "print(gpu_info)\n",
    "\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as datasets\n",
    "#from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import resnet\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_rtoBGhgSae"
   },
   "source": [
    "### Set arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "dvuxcmejkKt8",
    "outputId": "34615de0-d8a8-416d-8e27-8b95acf464e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(arch='resnet18', batch_size=128, bn_splits=8, cos=True, epochs=500, knn_k=200, knn_t=0.1, lr=0.03, moco_dim=128, moco_k=4096, moco_m=0.99, moco_t=0.1, results_dir='./IDC-cache/cache-moco-Cifar-IDC', resume='', schedule=[], symmetric=False, train_dir='/projectnb/dl523/projects/COVIDCT2/dataset/LUNA/', wd=0.0005)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Train MoCo on LUNA')\n",
    "\n",
    "parser.add_argument('-a', '--arch', default='resnet18')\n",
    "\n",
    "# lr: 0.06 for batch 512 (or 0.03 for batch 256)\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.03, type=float, metavar='LR', help='initial learning rate', dest='lr')\n",
    "parser.add_argument('--epochs', default=1000, type=int, metavar='N', help='number of total epochs to run')\n",
    "parser.add_argument('--schedule', default=[120, 160], nargs='*', type=int, help='learning rate schedule (when to drop lr by 10x); does not take effect if --cos is on')\n",
    "parser.add_argument('--cos', action='store_true', help='use cosine lr schedule')\n",
    "\n",
    "parser.add_argument('--batch-size', default=128, type=int, metavar='N', help='mini-batch size')\n",
    "parser.add_argument('--wd', default=5e-4, type=float, metavar='W', help='weight decay')\n",
    "\n",
    "# moco specific configs:\n",
    "parser.add_argument('--moco-dim', default=128, type=int, help='feature dimension')\n",
    "parser.add_argument('--moco-k', default=4096, type=int, help='queue size; number of negative keys')\n",
    "parser.add_argument('--moco-m', default=0.99, type=float, help='moco momentum of updating key encoder')\n",
    "parser.add_argument('--moco-t', default=0.1, type=float, help='softmax temperature')\n",
    "\n",
    "parser.add_argument('--bn-splits', default=8, type=int, help='simulate multi-gpu behavior of BatchNorm in one gpu; 1 is SyncBatchNorm in multi-gpu')\n",
    "\n",
    "parser.add_argument('--symmetric', action='store_true', help='use a symmetric loss function that backprops to both crops')\n",
    "\n",
    "# knn monitor\n",
    "parser.add_argument('--knn-k', default=200, type=int, help='k in kNN monitor')\n",
    "parser.add_argument('--knn-t', default=0.1, type=float, help='softmax temperature in kNN monitor; could be different with moco-t')\n",
    "\n",
    "# utils\n",
    "parser.add_argument('--train-dir',default='/projectnb/dl523/projects/COVIDCT2/dataset/LUNA/',type=str, help='path to training data (default:LUNA folder)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--results-dir', default='', type=str, metavar='PATH', help='path to cache (default: none)')\n",
    "\n",
    "'''\n",
    "args = parser.parse_args()  # running in command line\n",
    "'''\n",
    "args = parser.parse_args('')  # running in ipynb\n",
    "\n",
    "# set command line arguments here when running in ipynb\n",
    "#args.resume = 'cache-2022-04-14-16-10-30-moco/model_last.pth'\n",
    "#args.results_dir = './cache-2022-04-14-16-10-30-moco/'\n",
    "args.epochs = 500\n",
    "args.cos = True\n",
    "args.schedule = []  # cos in use\n",
    "args.symmetric = False\n",
    "\n",
    "args.results_dir = './IDC-cache/cache-moco-Cifar-IDC'\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygQeHtsngrC8"
   },
   "source": [
    "### Define data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68,
     "referenced_widgets": [
      "7f386ae8140f4f49bf9fabaef84b62fa"
     ]
    },
    "id": "AoliFX5AnBJ0",
    "outputId": "c3ed12b3-7e2a-4ff5-d204-446c2bf7bb63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#for CIFAR10 dataset\n",
    "\n",
    "class CIFAR10Pair(CIFAR10):\n",
    "    \"\"\"CIFAR10 Dataset.\n",
    "    \"\"\"\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index]\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            im_1 = self.transform(img)\n",
    "            im_2 = self.transform(img)\n",
    "\n",
    "        return im_1, im_2\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
    "\n",
    "\n",
    "\n",
    "# data prepare\n",
    "train_data = CIFAR10Pair(root='cifardataset', train=True, transform=train_transform, download=True)\n",
    "train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True, num_workers=16, pin_memory=True, drop_last=True)\n",
    "\n",
    "memory_data = CIFAR10(root='cifardataset', train=True, transform=test_transform, download=True)\n",
    "memory_loader = DataLoader(memory_data, batch_size=args.batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "test_data = CIFAR10(root='cifardataset', train=False, transform=test_transform, download=True)\n",
    "test_loader = DataLoader(test_data, batch_size=args.batch_size, shuffle=False, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for LUNA dataset\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.RandomResizedCrop(64, scale=(0.2, 1.)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LUNAPair(Dataset):\n",
    "    def __init__(self, root_dir,transform):\n",
    "        self.root_dir = root_dir\n",
    "        self.data_set = datasets.ImageFolder(self.root_dir)\n",
    "        self.transform = transform \n",
    "    def __len__(self):\n",
    "        return len(self.data_set)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data_set[idx][0]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            im_1 = self.transform(img)\n",
    "            im_2 = self.transform(img)\n",
    "            \n",
    "        return im_1, im_2\n",
    "    \n",
    "# data prepare\n",
    "train_data = LUNAPair(root_dir = args.train_dir,transform=train_transform)\n",
    "train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True, num_workers=16, pin_memory=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for COVIDCT dataset\n",
    "\n",
    "def read_txt(txt_path):\n",
    "    with open(txt_path) as f:\n",
    "        lines = f.readlines()\n",
    "    txt_data = [line.strip() for line in lines]\n",
    "    return txt_data\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.45271412, 0.45271412, 0.45271412],\n",
    "                                     std=[0.33165374, 0.33165374, 0.33165374])\n",
    "\n",
    "train_transformer = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.RandomResizedCrop((64,64),scale=(0.5,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(90),\n",
    "    # random brightness and random contrast\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "class CovidCTPair(Dataset):\n",
    "    def __init__(self, root_dir, txt_COVID, txt_NonCOVID, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            txt_path (string): Path to the txt file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        File structure:\n",
    "        - root_dir\n",
    "            - CT_COVID\n",
    "                - img1.png\n",
    "                - img2.png\n",
    "                - ......\n",
    "            - CT_NonCOVID\n",
    "                - img1.png\n",
    "                - img2.png\n",
    "                - ......\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.txt_path = [txt_COVID,txt_NonCOVID]\n",
    "        self.classes = ['CT_COVID', 'CT_NonCOVID']\n",
    "        self.num_cls = len(self.classes)\n",
    "        self.img_list = []\n",
    "        for c in range(self.num_cls):\n",
    "            cls_list = [[os.path.join(self.root_dir,self.classes[c],item), c] for item in read_txt(self.txt_path[c])]\n",
    "            self.img_list += cls_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = self.img_list[idx][0]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image1 = self.transform(image)\n",
    "            image2 = self.transform(image)\n",
    "        \n",
    "        return image1,image2\n",
    "\n",
    "    \n",
    "\n",
    "train_data = CovidCTPair(root_dir='/projectnb/dl523/projects/COVIDCT2/dataset/COVIDCT',\n",
    "                              txt_COVID='/projectnb/dl523/projects/COVIDCT2/dataset/COVID-Data-split/COVID/trainCT_COVID.txt',\n",
    "                              txt_NonCOVID='/projectnb/dl523/projects/COVIDCT2/dataset/COVID-Data-split/NonCOVID/trainCT_NonCOVID.txt',\n",
    "                              transform= train_transformer)\n",
    "   \n",
    "train_loader = DataLoader(train_data, batch_size=args.batch_size,drop_last=True, shuffle=True, num_workers=16)    \n",
    "#train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True, num_workers=16, pin_memory=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data. __getitem__(5)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for IDC Breast Cancer Dataset\n",
    "def read_txt(txt_path):\n",
    "    with open(txt_path) as f:\n",
    "        lines = f.readlines()\n",
    "    txt_data = [line.strip() for line in lines]\n",
    "    return txt_data\n",
    "DPATH = '/projectnb2/dl523/projects/COVIDCT2/dataset'\n",
    "label_map = {\"no IDC\": 0, \"IDC\": 1}\n",
    "torch.cuda.empty_cache()\n",
    "train_transformer = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomResizedCrop(36),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.8072870370812287, 0.6357799672097822, 0.7357258879210914],\n",
    "                             std = [0.14257688600883628, 0.21227747141835426, 0.15241009580979106])\n",
    "])\n",
    "\n",
    "class BreastCancerDataset(Dataset):\n",
    "    def __init__(self, root_dir, npy_IDC, npy_NonIDC, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Path to all data\n",
    "            npy_path (string): Path to the txt file with annotations.\n",
    "            transform: image augmentation \n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.npy_path = [npy_IDC, npy_NonIDC]\n",
    "        X_IDC = np.load(os.path.join(self.root_dir, self.npy_path[0]))\n",
    "        X_nonIDC = np.load(os.path.join(self.root_dir, self.npy_path[1]))\n",
    "        self.X = np.concatenate((X_IDC, X_nonIDC)) \n",
    "        \n",
    "        Y_IDC = np.ones(X_IDC.shape[0],dtype=np.int64)\n",
    "        Y_nonIDC = np.zeros(X_nonIDC.shape[0],dtype=np.int64)\n",
    "        self.Y = np.concatenate((Y_IDC, Y_nonIDC))\n",
    "        \n",
    "        assert np.issubdtype(self.Y.dtype, np.integer)\n",
    "        assert self.Y.ndim == 1\n",
    "        assert np.issubdtype(self.X.dtype, np.uint8)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        ## get image & label\n",
    "        image, label = self.X[index], self.Y[index]\n",
    "        \n",
    "        ## transform \n",
    "        if self.transform is not None:\n",
    "            image1 = self.transform(image)\n",
    "            image2 = self.transform(image)\n",
    "        if self.target_transform is not None:\n",
    "            label = self.target_transform(label)\n",
    "            \n",
    "        return image1, image2\n",
    "        \n",
    "npy_IDC_train = '/IDC/train_IDC.npy'\n",
    "\n",
    "npy_NonIDC_train = '/IDC/train_nonIDC.npy'\n",
    "\n",
    "train_data = BreastCancerDataset(DPATH, DPATH+npy_IDC_train,  DPATH+npy_NonIDC_train,\n",
    "                               transform=train_transformer)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=args.batch_size, drop_last=True,shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4437"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Two Augmented (query and key input ) Images from the singe image input ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ab40beaa5b0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import skimage.io \n",
    "for i,image in enumerate(train_loader): \n",
    "    print(i)\n",
    "    break \n",
    "\n",
    "skimage.io.imshow(image[0][15,0,:,:].numpy())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ab40bf52190>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAEYCAYAAAAtTS8wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAnFklEQVR4nO2df3AcZ5nnv8+MRr9lWz8cWbGdODjBiROCHUQSlhRLAux5s+FC7oALqYNwS613rzZ1Rx23XBbqCMfdVsGyQFEHFVZssgm7OUIgpJLiAkkIucvluA0xwQl2nF82dmxHtiJbsiTLGkkzz/0xLZDlfp4Z9UzPjNTfT1WXZvqZ9+1nunsevd3vt59HVBWEEJJEUrV2gBBCagUDICEksTAAEkISCwMgISSxMAASQhILAyAhJLE0lNNYRLYB+DqANIC/U9Uvep/v6e7Wc89ZF27MTtoNmztMUzYXTcaTM+Q/U7N5s42nGPK8SIltS4ttzDudOs3gmFxfmhrSpq3B+Vc5OjUbuj6qwioOYVZUuZc4Ozpqnymnz9ZM+DFoGjti99e1xrTlnbMh5e3p2WnbNDwUuv7g6DiOT079doOpFesUs1P2Nhagp449oqrbSm5QASIHQBFJA/gmgPcBOATgGRF5SFVfsNqce846/Pxnj4baUvufNbeVe/NVpu034/ZB9H7sJ7LhP9oXhibMNjNORMo7PwbrpAaAFU32ITg1YwfjhrT95TLOF292Itn5Xa2mravF/g4Pvjgcut7bXx7evozKTMR/lBlnP3t9Wv9gAaCj0T7mW/tWhK4/7ydfNts03/hp05ZFxrQ1Yca0NRx/zbQd/fv/Hrr+2r994PQVuSwyF91g9rOQ6Wf/rsezi0gzgCcBNKEQu36gqreVvIEQyhkBXg7gVVXdFzh3L4DrAZgBkBCSLCRl/+OMQBbANao6ISIZAE+JyI9V9Z+idlhOAFwL4OC894cAXFFGf4SQZYVUNABq4Z7D3CVaJljKulyIfRJERLaLyA4R2fHG8PG4N0cIqRekEABLXUrrUtIishPAEIDHVPXpclwsJwAeBrB+3vt1wbrTUNUBVe1X1f7VPV1lbI4QspQQAJJOl7wA6JkbLAXL9oV9qmpOVbegEG8uF5FLyvGxnEvgZwBcICLnoRD4bgRwUznOEEKWESJILe4SeFhV+0v5oKqOisgTALYB2BXFPaCMAKiqsyJyC4BHUJDB3Kmqu91GuVmkjen8A+vsmd6XDtoSmZXOLGpXiz0DNjieDV2fdmZQszl7VjYOWjLRBuje7Gvambmc9do50o3Lzl4Zuv7QmC2BOOzYouLNynqzuXH0mXHkJ167ielwdULHNR8w2+T3/G/T1t7Va9pmei80ba80nmva9n7oC6HrJ79/5tVoJe8BishqADNB8GtBQYHypXL6LEsHqKoPA3i4nD4IIcsUqewkCIA+AHcHErwUgPtU9UfldFhWACSEEAsBIKnKzbOq6vMAtlasQzAAEkJio+IjwIrDAEgIiYfKXwJXHAZAQkhsMAASQpKJyJy+r26pagAcz2fwRLYv1DY5YSchGDMSFwC+nMJ7qN7KyOHJPbwsHt62vAwzubz9QLonyfFocZIvbOhsMW2rmu12TdNjdp+PfCN0/UWXvN1sk73kfabtheFTpu21E/bxHjOy0gDRkxN47TzaG+19udFJOrEpdSx0/WzTBrNNQ8r235O6TDuqrvNzB2xbuOoJK9K5094XJkEYAAkhSYT3AAkhyWXRT4JUHQZAQkg8CC+BCSEJRagDJIQkGQZAQkgy4SRI6UxM50ybJyPJOjYv64bVzpMveDIYp3yH62Peyyii0Z6j7Gy2223KD9oNX37ZNOU2X23aBq/9i9D15598yWyTeuRbpm3rOW82bVvOe5tpO9nolpQw8WqonHQObItTX6URtiRHnIJDcuJk6HptX222mTrLlro0DdnHVA441Ss6Vpmm3AW/F25IL8y+xABICEkqAgqhCSHJhJMghJDkwnuAhJAkwwBICEksqYjPs1cLBkBCSCyICIQB8HfM5vMYOhkuAXClIk5GjqjFbpoMCYNXUMjDyyLT6mRn8bKNzOTtfeIV6/H+676UCs/GAwAnemypxQu77ZrO1vH5vym7IE/Thg+Ztgt72kzbmxqbTFvH8b2mbfLx+0xbapVdrrXnQlt2k58Kl6wU5exNpun1tvNC14+esGViDSlbVtPevtH242Lb5sl/ZsbCj/dUiIvi/C7qAY4ACSHxIEDa0UrWAwyAhJBYKOQD5AiQEJJIxH16qh5gACSExINwBEgISTAMgISQRCKyzHWAIrIfwDiAHIBZVe33Pq+wJROuHMSRfHgymIxTld6SmETdlocnkfFsPrafI6fsQktDE1nTls05KW0iMDljSzc8247XTzg2e3trO2wZz3s//GnTNvL1W03byrwjP1lry0hkw1tM22iLLQ9aaQSMvpkjZhttaDZtzmmCfFu3aTs8YTccnw7PdBP225b6ngSuyAjwalUdrkA/hJBlBnWAhJBEIiJ1fwlc7gBVATwqIr8Uke1hHxCR7SKyQ0R2jI/YTxMQQpYfkpKSl1pQ7gjwKlU9LCJnAXhMRF5U1Sfnf0BVBwAMAMB5my+N9pwZIWRJUu+zwGWNAFX1cPB3CMADAC6vhFOEkGWAFMpIlLrUgsgjQBFpA5BS1fHg9R8A+ILbBnZdDa8ug1c3w5u1BRY/q2klSQCK1R9xvHBmuL0D780Qd7YsrL9QGhFzPbj72ZrBb3b2pfe9vf3lMeXMYo/O2L50Xm6LF+Tt/9y0TWdaTVsmO2bauo7bdTp0PPw2kbS2m22yz//ctKVX2jO9DRvfatrWdZ1j2jqbw2edF/52Kv0onIisB/AdAL0o3H4bUNWvl9NnOZfAvQAeCGZ5GgD8D1X9STnOEEKWExW/tzcL4FOq+qyIdAD4pYg8pqpOdSefyAFQVfcBsP+FEEKSTYWF0Ko6CGAweD0uInsArAVQ/QBICCHFiEsHKCIbAGwF8HQ5/TAAEkJioXAPcFFNekRkx7z3A4GK5PR+RdoB3A/gk6pq32wtAQZAQkg8LP4SeLjY47QikkEh+N2jqj8sxz2AAZAQEiMVngUWAHcA2KOqX61En3UTAKPW4vASFERJbOBJT6Juy8OTz3jSIE8G4yUamKlwwgPAlrt4UhdPUuQlxvDwkkA8e2TCtDWfe53d6ZAnfbJrgvS1t5i2davPN23p5qOh62d/9bjZZnbUfsKq8c1bTFtuxRrTBqceTWuj8ds5Y41U+h7gOwF8FMCvRWRnsO4zqvpw1A7rJgASQpYXlU6HpapPoXBrsWIwABJCYqPeH4VjACSExIIIkGYAJIQkFQZAQkgiEQgDICEkofAS+HS8miAe0VPl2NuaMmQYqUxlZTXlEDVziycxidpnFKJmdYmaRaaj0T6de9saTZt3XD1JkefL+LR9DPZMmyY0N5wVun7jFdebbdrGBk1bPmPLcbTRzmZzbNr+bhNj4XKj7IL9KGAAJIQkFBGggQGQEJJEOAIkhCQX4SQIISShFEaA9V0YmAGQEBIbHAESQhIJnwRZgECQMYbEM2JLEWaczBRRpSkZ45nqqNKNOIiaFcWTwXiyDnd/RciE4xWY8iRKHp7UZf3K8GI9xX2x91dng511Z2Nnk2nzfvcHDRkJAKixX16btSUr3T2bTNu08/sYnbDPhYnpWdNmsVBiJRA0pnkJTAhJIBwBEkISDQMgISSRUAhNCEksTIZACEk0DICEkESyLCZBROROANcBGFLVS4J1XQC+B2ADgP0APqyqI8U3p5GlHRZRs7BYmTw8mYhXMMnLDOJJazwZT3ujLSFoduQFUc+5yJIiwxZVUrS2w5azrF1h21oz9j7x9uWRCS97jv0dTnmFnZy0O8ed4k09reGyG++7tU0OmTa0hmeXAYCJaa941uIz5Cz8bS+FZ4FLEencBWDbgnW3AnhcVS8A8HjwnhBCfkcwAix1qQVFA6CqPglgYd296wHcHby+G8AHKusWIWSpMzcJUs8BMOo9wF5VncvCeARAb4X8IYQsI+r9ErjsSRBVVRH7OTYR2Q5gOwB0r1lb7uYIIUuEpTAJEvVBvaMi0gcAwV/zLqyqDqhqv6r2r+jsirg5QshSY24SpJ4vgaMGwIcA3By8vhnAg5VxhxCybFgCkyClyGC+C+DdAHpE5BCA2wB8EcB9IvIJAAcAfLiUjeXVnkKPKpmIWjDJ2p5XkCdKf4AvKYiKJ0vxTqY4ijdZeMdma1+HaevvsY+BOn3uHrFlHS3Occ04kiLvdBidsrd3ImtnU/EyraxuC5fBeEWWutq7Tdspp50nSfNsU7nwPhf+BgTiSsfqgaIBUFU/YpjeU2FfCCHLjOgVHasDnwQhhMSCAKjiBUckGAAJIfEgQKrOZ4EZAAkhsVAYAdZ3AKzvfNWEkCVNSqTkpRRE5E4RGRKRXRXxrxKdEELIQubuAZa6lMhdODM3QWSqegmcV3UL9lh40g1PfuL9VzGLMzkFmOLAz7Ji/3+6on3CtPW124LzvSOnTNvIKVue0eD4efHq9tD1a9rt0+uNSXtbkh0zbUjbRYqARsdms3GV3e7FY1OmzTu/vOJNnq23NdzWPvyS2SaXXm3akF5lmxz/s+pkyHEy3ZyGSMXvAarqkyKyoVL98R4gISQWBJTBEEISzCJlMD0ismPe+wFVHaisR6fDAEgIiYUII8BhVe2PyZ1QGAAJIfGwjLPBEEKIy9wIsMIymO8C+H8ANonIoSAfQWQ4AiSExEalH4VzchNEoqoBUEQiZSOJmk0lk7ZtURTqlnQG8LNnALat1XHyrWvsjCmp6ddN2/pXHzJt53Z02n122gV08k22L/m2lvD+JuxiPe2O3EgbbFnKMUfWcfTkSdPm7Wcv44t3XKMWkeppsX92rSdeC13/jUv/ldnmY1/+l6at82OfM22HxqdNmyshM77bwrWC0kd2tYIjQEJIPCyBe4AMgISQWCjcA6y1Fz4MgISQ2Kj3ZAgMgISQWOCTIISQxCJS3RIMUWAAJITExDKoCVIt/Gl3u11UiUyUrC+eJMI70Cub7d28tqPZtK3vsDOf5He9aNomX9pt2qaO2ZlWGle0mrb2S95q2vC2PwpdPd52ttnEGxg0a9Y22klkcGzSlnV0Ndv7ciZvn2CefCYqJ2fsc6+zNVymdLZzDu2660nT1n/2Habt0qs/atoOTS5eBrNwxpeXwISQ5CKAk9GtLmAAJITEAkeAhJBEU+fxjwGQEBIfqTMekKsvGAAJIbEg4AiQEJJglvyjcCJyJ4DrAAyp6iXBus8D+BMAbwQf+4yqPlysL1U1ZStxSF08poziTF7BGu9gdrbYMguvT09m8fqErfnY+Kat9vbOPt+0tTeGZ24BAMna2VRSM3YxpWxDuJRnaipntvFqY0mzLQ2amJkxbSua7P18doedYeaNSbtPrzCVl0XGqxuUzdlffiQTXmDqhie+abbRnnNN22TbGtPmJSpY32IXg5rJh+/nM6RgUv8jwFImqe9CeBm6r6nqlmApGvwIIclCIEgtYqkFRUeAlS5DRwhJDsthBGhxi4g8H1RqN7Nsish2EdkhIjsmRo+XsTlCyFIjJaUvNfEvYrvbAWwEsAXAIICvWB9U1QFV7VfV/vZVdsFuQsjyQxax1IJIs8CqenTutYh8G8CPKuYRIWRZsBSeBIk0AhSRvnlvbwCwqzLuEEKWEyKlL7WgFBnMdwG8G4Wq7YcA3Abg3SKyBYVqP/sB/GkpG/OKIkXNBhMVS37S5GgbelptqYsnZ4n6X1CdYkpPjNhylpmcLSPx8rN1tti3KNZ22zKS3jdeDV2/oseW40xM21KQvSN2Nphup6DQtd22jEfVluSMNdgFn05M2VKkZudc8c4jTyIzOBEuyRnuusxss6LBPvd6ncw6MmXbNG2f6xccfSZ0fdPsmfu/znMhlDQLHFaGzs6xQwghmBvZ1fclMJ8EIYTExpJ/EoQQQqJS5wNABkBCSDwIlsE9QEIIiQrvARJCkkkNn/AolaoGwJaGFC45K1xykPXSgzj4cgNbb3D0ZHgBnVkn88yQ0QYAco62wZM9eIWWPJmFJ63ZsMqWyHQ5WWusDDkAcPCE/d27e8PlLvtH7TYjp+wMLOPTtmTl8JidpeRA8yrTttIpiuTh7a8VTfbx8QofvXHS/u5Wca1M2pa6TDnZZVIv/9y0Te+z5btNV1xr2nIb3hZuaDyzqFadxz+OAAkh8VB4EqTWXvgwABJCYqPe7wHW+yQNIWSJMjcCrGQ2GBHZJiIvicirInJruT4yABJCYqOS2WBEJA3gmwD+EMBmAB8Rkc3l+MdLYEJITEils8FcDuBVVd0HACJyL4DrAbwQtcOqBsDpnOLgifBZvG4n0YA1MwYAgxP2A93eTKPV5+SMPQM57cy2tTXas3SZVLSBtudLu1NnZGWzUxuj3bYNn7K3t8eZff0/B8MTBniz2K8Z5wEAvKXXTk5wUeoN05aeGDZtu3CRaWtyEkRMOrO5u9+YMG0rnfokl/a2mbYWY5/td2bhTzqJJV47512mrWvT1aYtDadOyvC+0PWSW+Bj5bO8rAVwcN77QwCuKKdDjgAJIbEgqhBH5hVCj4jsmPd+QFUHKuzWaTAAEkLiQxel7x1W1X7HfhjA+nnv1wXrIsMASAiJDVlcACzGMwAuEJHzUAh8NwK4qZwOGQAJITGhQN6+r7zo3lRnReQWAI8ASAO4U1V3l9MnAyAhJB5UF3sJXEKX+jCAitUhZwAkhMRGhS+BK05VA2BeFePT4ZIJT/LhJQzw2nl4Eg2LDkfa4BFVWtOYtn30+vRU9QfGbHnDa6OnTJt3DDZ1n/kQPACs0VGzTX9ft2lLnzxm2vIZswQ19slZpu2Ek8jCS6jhcfHq9kjtjp2y64ysbQ+Xg3lSHVsIBoxl7fPES9jgMa7nhK6f0JC6MQyAhJBkUvlL4ErDAEgIiQcFAyAhJKkokGcAJIQkFE6CEEKSCwMgISSRqBaWOqZoABSR9QC+A6AXhduaA6r6dRHpAvA9ABsA7AfwYVUd8fpS2HU6otYEiVo3w/Ij48gNPKL670lrvCw4np/ZWfukmzBkSMXY2BUudQGAvtmh0PUpR84y1WLLYPbO2NlgJidtWcfKJtOEC7rsOinDjizFO65eDZWoWDKl8aztY9TzJO/Uv/F+Oz1G5qaGMP1VnY8ASxFAzQL4lKpuBnAlgD8PkhDeCuBxVb0AwOPBe0II+S2i+ZKXWlA0AKrqoKo+G7weB7AHhbxc1wO4O/jY3QA+EJOPhJAlSaADLHWpAYu6BygiGwBsBfA0gF5VHQxMR1C4RCaEkN9R55fAJQdAEWkHcD+AT6rq2PxqT6qqIhJ6Q0FEtgPYDgBda84uz1tCyNIhhmQIlaakhyBFJINC8LtHVX8YrD4qIn2BvQ9A6J1wVR1Q1X5V7W9fZd/4JoQsLwTL4B6gFIZ6dwDYo6pfnWd6CMDNweubATxYefcIIUuafL70pQaUcgn8TgAfBfBrEdkZrPsMgC8CuE9EPgHgAIAPF+tIVU3pQFQ5iydTmM7Zkgkr04qXGSSq1CVqtpEZ56RY0RySeaOEdr3tdruzO2wdSXODfQxmm/tC1z93apXZZvjACdM2MmVnrIma/SeKJKpYu6jyk+5W+xj0tYcfA+/3MePIWTybRyrlSanCbfkzVi8DHaCqPgW7bOd7KusOIWTZwGQIhJAkw2eBCSEJpf5ngRkACSHxwQBICEkkWtmqcHHAAEgIiQ1lQtTTsaQDXmYNT27gSUwqLWmJmikmKk1OUSRLLgEA3S3h2ToAoC1j9+kVyRk6aWcj+dlvwpMAxSFZ8fAkH5l0pC7d88Q7H3Jq2w6PTZm2oYnwEkd9jkTpop420zblFN2acrIGecqtM+UuBc48bBwBEkKSioIBkBCSTBQKdR5GqAcYAAkh8aBgUSRCSFLhPUBCSFJRhTIAEkISCy+BSyN6AaDKZmiJ2l9UiYyX5aOz2ZazeBKTA6O2zMJrNzw5bdqyjpzCkq34spTKH1Ovz6gZhTzikN1MTIf78toJ+5i++yynw0Y780w21Ww3c2QwVuGmM/cwR4CEkKSiCszaqc3qgWiJ6gghpBgKaC5X8lIuIvIhEdktInkR6S+lDUeAhJCYqPos8C4A/wLA35bagAGQEBIPVU6GoKp7AEAW8SglAyAhJDaYDIEQklAWPQLsEZEd894PqOrA/A+IyE8BrAlp+1lVXXRhtqoGwOaGNDYZmSu8DBkT09GG0Z4swiqEE1VKkUl5Mp5oGWui+A8ABx3JhFcwKSpR9qVHHFl3vP3lbS+qlKfSeP7PNq0wbS2vP2fa9BXbJm12n9j0R7bttA1gsQFwWFXdyQtVfe9iOiwGR4CEkFhQaN1fAlMGQwiJh7kRYKlLmYjIDSJyCMA7APxPEXmkWBuOAAkhMVH1WeAHADywmDYMgISQeAiE0PUMAyAhJCaUyRAIIQlmqSdDEJH1AL4DoBeF25oDqvp1Efk8gD8B8Ebw0c+o6sNeX00NgvNWhWegeMtZdmGXkSl7J/5m9JRpOzIeXmCm4Etl53+8bDYep5zsLGNTdiEij6hSF0/m48kwovRXbTz/o0pdvHYevvQpfH17o/1TzTp+yHNPmbbhX71g2tZc937T1mGkikkvlIItk3yAswA+parPikgHgF+KyGOB7Wuq+jfxuUcIWcrUuwymaABU1UEAg8HrcRHZA2Bt3I4RQpY4qlAnj2Q9sKjrQBHZAGArgKeDVbeIyPMicqeIdBpttovIDhHZcfzYsfK8JYQsGVQBzeVLXmpByQFQRNoB3A/gk6o6BuB2ABsBbEFhhPiVsHaqOqCq/ara39XdXb7HhJAlQuFJkFKXWlDSLLCIZFAIfveo6g8BQFWPzrN/G8CPYvGQELI0CUaA9Uwps8AC4A4Ae1T1q/PW9wX3BwHgBhSSERJCyG9Z8gEQwDsBfBTAr0VkZ7DuMwA+IiJbUJDG7Afwp8U6ys7msXckXLZy5doOs1261a4wc/yU/RWGJmwZzMqm8HbnrGox24xnbVnKoLOtCadd3lFSeHIWT2JSaTlLVOKQnsRB5AxAEQsfuQXAjKxCnnRm55GTpu2qq/+1aetd/YRpS68+27R1NYd/8YYzVDCK/FJ/EkRVn0JYwSfA1fwRQsiSl8EQQkgkloAMhgGQEBIbDICEkESiWv8JURkACSGxkecIkBCSSJaDDrCSZHOK/Ub2lqGT02a7DY40ZWVzxrS9pdeW1gxPhm+vu8XWNniFj044Uhcvq0vUzC3ZWbtd1Ew3nmwlSmaXqIWIomaR8eQlzqFzpUienzmnoden58sZGVUCPCmVd379Xt9K0zb1si3dPfqP3zdtmbbwjE7Thw+cvoKTIISQpKKgDIYQklQ4AiSEJBkGQEJIMlEgz0tgQkgSUfASmBCSVFSRn5mptRcudRMAOx05y2Vr7IJJ6bwtn3l5zJaDWEVmxrL2f6y2jN1fc9q2eVKXqIV1Wp1UJFGlNR6VLorkZkSJmA0mqizFwzs+uciZdZxCS4a8yZM9eedCatfPTNvTX/qxaTt63C429nPDdgQTp6+gDpAQklx4CUwISSiqfBSOEJJYmAyBEJJUeA+QEJJYFNCIk3zVggGQEBILCuU9wPkIbGnExq5Ws934tL0TByfs/zB7RyZN28bO8O1ZWWIAYNKRG0zOVL74i5uJpIrFjYphSUWiFg1KOb+ZqEWkpiLKSOIo7ORJWqL0t3ZFk2nLvXzYtK2+2K7TfdVHrjFtN115bej6Z95/0+krFNCo+qMIiMiXAbwfwDSAvQD+jaqOem2i5U0ihJASyOe05KUCPAbgElW9FMDLAP6yWANeAhNCYkGrPAmiqo/Oe/tPAD5YrA0DICEkHlRrOQnyxwC+V+xDDICEkNhY5KVtj4jsmPd+QFUH5n9ARH4KYE1I28+q6oPBZz4LYBbAPcU2yABICImHxV8CD6tqv9ul6ns9u4h8HMB1AN6jWnymsGgAFJFmAE8CaAo+/wNVvU1EzgNwL4BuAL8E8FFVtadQAbQ1pnHFulWhtuOn7KwRzx0ZN20nsna7JidBgTUT59XT8GaIPVvU2UKPKMkJysFLbGBNovqJHhybcwyifm8v+ULUPqMeuyh9evv/3b32/pr8X/Ys8MX/4Y9N23Mb/plp23V0InT9sZnTTwQFkK/uLPA2AJ8G8PuqaktA5lHKLHAWwDWq+lYAWwBsE5ErAXwJwNdU9XwAIwA+EclrQsjyJLgHWOpSAb4BoAPAYyKyU0S+VaxB0RFgMIycC/mZYFEA1wCYE/7cDeDzAG5fvM+EkOVKNYXQwWBsUZSkAxSRtIjsBDCEgtZmL4BRVZ2rx3cIwNrFbpwQsnzR4FG4Ko4AF01JkyCqmgOwRURWAXgAwIWlbkBEtgPYDgB9a9dHcJEQsiRZAs8CL+pJkOCxkicAvAPAKhGZC6DrAITebVXVAVXtV9X+zm770RtCyHKj8CxwqUstKBoARWR1MPKDiLQAeB+APSgEwjml9c0AHozJR0LIUiR4FrjUpRaUcgncB+BuEUmjEDDvU9UficgLAO4Vkf8G4FcA7ijWUS6vGM/Ohtr2OYkL4kg0YOE9qN7T2mjaxqdtHyccm4cnfbD2I1CQG1V6e1FkJHHIRDwfPZoaKpucAPC/n+dnq1Nb5qy28HPsvec6dXH2/cLe1qaLTZu+7TrT9tzu4/b2SjwGikULoatOKbPAzwPYGrJ+H4DL43CKELIMUNYEIYQkmHqfBGEAJITEQqEoEgMgISShsCgSISSZaMUSncYGAyAhJB6WgBC6qgFwajaPl4ZPhto8qYtX/8Kr55BxssFYfV7eZW9rBLYM5uhJNxFOJDIpW27Q6Hw3LwuLty89eUNUyUeUbXl4/keVs3gZgKLS7PT5JqMeDQBc1NMSuv7VMTvrUWq1nUGqa739Ez8+bkupvONTaj0aBctiEkKSCidBCCHJpaYp8UuCAZAQEguq1U/cu1gYAAkhsVFP9avDYAAkhMSCApiuUZKDUmEAJITEggKo81uA1Q2AeVVT7uLJLDJwbI5UxOOVY+FynInpJrNNe+OUaRubsiUFUbOiZB0JQVRpUNR7MlOOxMSTfETxY/Pq9kX3BwAvGhKrYnjymfUrm01bR6P98+kxsroU49G9I6Hrx6ejSVas7DIAsHaF/d1u2txp2mZ+8Deh62+fGj7tvSovgQkhCYYjQEJIIlEoR4CEkGTCe4CEkETDAEgISSScBCGEJBqOACtAJmXLLDyd5YQjHbAypuwfPWW2aXdkDzOOZMXz31PxeDIYT3oSx+NHUaQuXpYYT3riSTdaHD9OOFKklLOju5ozpm140s7yMziRjWTziFIAzGsz5GQpWtlsn885seVg6RWrDMPp8qvCPcD6joBLIgASQpYenAQhhCQajgAJIYmkMAlSay98GAAJIbHBESAhJJEogPpOiM8ASAiJjeo+Cici/xXA9SjE3SEAH1fV1702RQOgiDQDeBJAU/D5H6jqbSJyF4DfB3Ai+OjHVXWn11c6JWhvDM9U4slZomZTATz5SfgGPUlBSmybl4HFk8icyNrSDU96EqUQEeBLZLw+vYwjMxWu/fqrwXHT1uCcCyOn7MJB3vc+PGZn+fGIY19afXpSKu/34RV88s7Z8WnnmF69PXR1ruMfTntfg1ngL6vqfwYAEfl3AD4H4M+8BqWMALMArlHVCRHJAHhKRH4c2P5CVX9QjseEkOVJtXWAqjo2721b4IJL0QCoqgpgInibCZb6vrNJCKk9NZgFFpG/AvAxFK5Mry72+ZLk/SKSFpGdKFxXP6aqTwemvxKR50XkayLh0nER2S4iO0Rkx/jI8ZK+BCFk6TM3Aix1AdAzFyuC5YxrbRH5qYjsClmuBwBV/ayqrgdwD4BbivlY0iSIquYAbBGRVQAeEJFLAPwlgCMAGgEMAPhPAL4Q0nYgsOO8zZdy5EhIgljkCHBYVe0q7wBU9b0l9nUPgIcB3OZ9aFEPeKrqKIAnAGxT1UEtkAXw9wAuX0xfhJDlTYQRYFmIyAXz3l4P4MVibUqZBV4NYEZVR0WkBcD7AHxJRPpUdVBEBMAHAOyK5jYhZDlSg1ngL4rIJhRkMAdQZAYYAESLRF4RuRTA3QDSKIwY71PVL4jIzwCsBiAAdgL4M1WdMDsq9PVG4BgA9AAYdj5eTerFl3rxA6gfX+rFD6B+fKkXP4DTfTlXVVfPGUTkJ4G9VIZVdVslnStG0QAY24ZFdhS73q8W9eJLvfgB1I8v9eIHUD++1IsfQH35EoXFJ3kjhJBlAgMgISSx1DIADtRw2wupF1/qxQ+gfnypFz+A+vGlXvwA6suXRVOze4CEEFJreAlMCEksNQmAIrJNRF4SkVdF5NZa+DDPl/0i8msR2SkiO6q43TtFZEhEds1b1yUij4nIK8Hfzhr68nkRORzsl50icm0V/FgvIk+IyAsisltE/n2wvqr7xfGjFvukWUR+ISLPBb78l2D9eSLydPAb+p6I2JWk4vXjLhH5zbx9siVOPyqOqlZ1QUFPuBfAm1B4jO45AJur7cc8f/YD6KnBdt8F4DIAu+at+2sAtwavbwXwpRr68nkA/7HK+6QPwGXB6w4ALwPYXO394vhRi30iANqD1xkATwO4EsB9AG4M1n8LwL+tkR93AfhgNfdJJZdajAAvB/Cqqu5T1WkA96Lw2EqiUNUnASzMDnE9CqJzBH8/UENfqo4WHq98Nng9DmAPgLWo8n5x/Kg6WiAsG9M1AOZS0VVjn1h+LGlqEQDXAjg47/0h1OjkClAAj4rIL8OyT1SZXlUdDF4fAdBbS2cA3BJk+7mzWpfjc4jIBgBbURhp1Gy/LPADqME+WZiNCYUrqFFVncumW5XfUDlZoeoVToIAV6nqZQD+EMCfi8i7au0Q8Ns8jLX8D3s7gI0AtgAYBPCVam1YRNoB3A/gk3p6ksuq7pcQP2qyT1Q1p6pbAKxD4Qrqwmpst5gf87JCXQjg7QC6UMgKtWSoRQA8DGD9vPfrgnU1QVUPB3+HADyA2ma1OSoifQAQ/B2qlSOqejQ44fMAvo0q7RcpZB2/H8A9qvrDYHXV90uYH7XaJ3Po77IxvQPAKhGZS2ZS1d+QLqOsULUIgM8AuCCYxWoEcCOAh2rgB0SkTUQ65l4D+APUNqvNQwBuDl7fDODBWjkyF3ACbkAV9kuQWegOAHtU9avzTFXdL5YfNdonq6WQhxPzsjHtQSEAfTD4WDX2SZgfL877x7Q0s0LVYuYFwLUozKztBfDZWs0AoTAT/Vyw7K6mLwC+i8Jl1AwK93A+AaAbwOMAXgHwUwBdNfTlHwD8GsDzKASgvir4cRUKl7fPo5BhaGdwrlR1vzh+1GKfXArgV8E2dwH43Lxz9xcAXgXwfQBNNfLjZ8E+2QXgHxHMFC+VhU+CEEISCydBCCGJhQGQEJJYGAAJIYmFAZAQklgYAAkhiYUBkBCSWBgACSGJhQGQEJJY/j+LBiI2kZUs0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "skimage.io.imshow(image[1][15,0,:,:].numpy())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsAVAtRoiBbG"
   },
   "source": [
    "### Define base encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "bNd_Q_Osi0SO"
   },
   "outputs": [],
   "source": [
    "# SplitBatchNorm: simulate multi-gpu behavior of BatchNorm in one gpu by splitting alone the batch dimension\n",
    "# implementation adapted from https://github.com/davidcpage/cifar10-fast/blob/master/torch_backend.py\n",
    "class SplitBatchNorm(nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, num_splits, **kw):\n",
    "        super().__init__(num_features, **kw)\n",
    "        self.num_splits = num_splits\n",
    "        \n",
    "    def forward(self, input):\n",
    "        N, C, H, W = input.shape\n",
    "        if self.training or not self.track_running_stats:\n",
    "            running_mean_split = self.running_mean.repeat(self.num_splits)\n",
    "            running_var_split = self.running_var.repeat(self.num_splits)\n",
    "            outcome = nn.functional.batch_norm(\n",
    "                input.view(-1, C * self.num_splits, H, W), running_mean_split, running_var_split, \n",
    "                self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits),\n",
    "                True, self.momentum, self.eps).view(N, C, H, W)\n",
    "            self.running_mean.data.copy_(running_mean_split.view(self.num_splits, C).mean(dim=0))\n",
    "            self.running_var.data.copy_(running_var_split.view(self.num_splits, C).mean(dim=0))\n",
    "            return outcome\n",
    "        else:\n",
    "            return nn.functional.batch_norm(\n",
    "                input, self.running_mean, self.running_var, \n",
    "                self.weight, self.bias, False, self.momentum, self.eps)\n",
    "\n",
    "class ModelBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Common CIFAR ResNet recipe.\n",
    "    Comparing with ImageNet ResNet recipe, it:\n",
    "    (i) replaces conv1 with kernel=3, str=1\n",
    "    (ii) removes pool1\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=128, arch=None, bn_splits=16):\n",
    "        super(ModelBase, self).__init__()\n",
    "\n",
    "        # use split batchnorm\n",
    "        norm_layer = partial(SplitBatchNorm, num_splits=bn_splits) if bn_splits > 1 else nn.BatchNorm2d\n",
    "        resnet_arch = getattr(resnet, arch)\n",
    "        net = resnet_arch(num_classes=feature_dim, norm_layer=norm_layer)\n",
    "\n",
    "        self.net = []\n",
    "        for name, module in net.named_children():\n",
    "            if name == 'conv1':\n",
    "                module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            if isinstance(module, nn.MaxPool2d):\n",
    "                continue\n",
    "            if isinstance(module, nn.Linear):\n",
    "                self.net.append(nn.Flatten(1))\n",
    "            self.net.append(module)\n",
    "\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        # note: not normalized here\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJFOHlOBpLay"
   },
   "source": [
    "### Define MoCo wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lzFyFnhbk8hj",
    "outputId": "973e6816-de1b-4331-8da6-e63227467b17"
   },
   "outputs": [],
   "source": [
    "class ModelMoCo(nn.Module):\n",
    "    def __init__(self, dim=128, K=4096, m=0.99, T=0.1, arch='resnet18', bn_splits=8, symmetric=True):\n",
    "        super(ModelMoCo, self).__init__()\n",
    "\n",
    "        self.K = K\n",
    "        self.m = m\n",
    "        self.T = T\n",
    "        self.symmetric = symmetric\n",
    "\n",
    "        # create the encoders\n",
    "        self.encoder_q = ModelBase(feature_dim=dim, arch=arch, bn_splits=bn_splits)\n",
    "        self.encoder_k = ModelBase(feature_dim=dim, arch=arch, bn_splits=bn_splits)\n",
    "\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data.copy_(param_q.data)  # initialize\n",
    "            param_k.requires_grad = False  # not update by gradient\n",
    "\n",
    "        # create the queue\n",
    "        self.register_buffer(\"queue\", torch.randn(dim, K))\n",
    "        self.queue = nn.functional.normalize(self.queue, dim=0)\n",
    "\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _momentum_update_key_encoder(self):\n",
    "        \"\"\"\n",
    "        Momentum update of the key encoder\n",
    "        \"\"\"\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, keys):\n",
    "        batch_size = keys.shape[0]\n",
    "\n",
    "        ptr = int(self.queue_ptr)\n",
    "        assert self.K % batch_size == 0  # for simplicity\n",
    "\n",
    "        # replace the keys at ptr (dequeue and enqueue)\n",
    "        self.queue[:, ptr:ptr + batch_size] = keys.t()  # transpose\n",
    "        ptr = (ptr + batch_size) % self.K  # move pointer\n",
    "\n",
    "        self.queue_ptr[0] = ptr\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _batch_shuffle_single_gpu(self, x):\n",
    "        \"\"\"\n",
    "        Batch shuffle, for making use of BatchNorm.\n",
    "        \"\"\"\n",
    "        # random shuffle index\n",
    "        idx_shuffle = torch.randperm(x.shape[0]).cuda()\n",
    "\n",
    "        # index for restoring\n",
    "        idx_unshuffle = torch.argsort(idx_shuffle)\n",
    "\n",
    "        return x[idx_shuffle], idx_unshuffle\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _batch_unshuffle_single_gpu(self, x, idx_unshuffle):\n",
    "        \"\"\"\n",
    "        Undo batch shuffle.\n",
    "        \"\"\"\n",
    "        return x[idx_unshuffle]\n",
    "\n",
    "    def contrastive_loss(self, im_q, im_k):\n",
    "        # compute query features\n",
    "        q = self.encoder_q(im_q)  # queries: NxC\n",
    "        q = nn.functional.normalize(q, dim=1)  # already normalized\n",
    "\n",
    "        # compute key features\n",
    "        with torch.no_grad():  # no gradient to keys\n",
    "            # shuffle for making use of BN\n",
    "            im_k_, idx_unshuffle = self._batch_shuffle_single_gpu(im_k)\n",
    "\n",
    "            k = self.encoder_k(im_k_)  # keys: NxC\n",
    "            k = nn.functional.normalize(k, dim=1)  # already normalized\n",
    "\n",
    "            # undo shuffle\n",
    "            k = self._batch_unshuffle_single_gpu(k, idx_unshuffle)\n",
    "\n",
    "        # compute logits\n",
    "        # Einstein sum is more intuitive\n",
    "        # positive logits: Nx1\n",
    "        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)\n",
    "        # negative logits: NxK\n",
    "        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])\n",
    "\n",
    "        # logits: Nx(1+K)\n",
    "        logits = torch.cat([l_pos, l_neg], dim=1)\n",
    "\n",
    "        # apply temperature\n",
    "        logits /= self.T\n",
    "\n",
    "        # labels: positive key indicators\n",
    "        labels = torch.zeros(logits.shape[0], dtype=torch.long).cuda()\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss().cuda()(logits, labels)\n",
    "\n",
    "        return loss, q, k\n",
    "\n",
    "    def forward(self, im1, im2):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            im_q: a batch of query images\n",
    "            im_k: a batch of key images\n",
    "        Output:\n",
    "            loss\n",
    "        \"\"\"\n",
    "\n",
    "        # update the key encoder\n",
    "        with torch.no_grad():  # no gradient to keys\n",
    "            self._momentum_update_key_encoder()\n",
    "\n",
    "        # compute loss\n",
    "        if self.symmetric:  # asymmetric loss\n",
    "            loss_12, q1, k2 = self.contrastive_loss(im1, im2)\n",
    "            loss_21, q2, k1 = self.contrastive_loss(im2, im1)\n",
    "            loss = loss_12 + loss_21\n",
    "            k = torch.cat([k1, k2], dim=0)\n",
    "        else:  # asymmetric loss\n",
    "            loss, q, k = self.contrastive_loss(im1, im2)\n",
    "\n",
    "        self._dequeue_and_enqueue(k)\n",
    "\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YXcpXBwi8KV"
   },
   "source": [
    "### Define train/test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "DBKFqboqnqty"
   },
   "outputs": [],
   "source": [
    "# train for one epoch\n",
    "def train(net, data_loader, train_optimizer, epoch, args):\n",
    "    net.train()\n",
    "    adjust_learning_rate(optimizer, epoch, args)\n",
    "\n",
    "    total_loss, total_num, train_bar = 0.0, 0, tqdm(data_loader)\n",
    "    for im_1, im_2 in train_bar:\n",
    "        im_1, im_2 = im_1.cuda(non_blocking=True), im_2.cuda(non_blocking=True)\n",
    "\n",
    "        loss = net(im_1, im_2)\n",
    "        \n",
    "        train_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        train_optimizer.step()\n",
    "\n",
    "        total_num += data_loader.batch_size\n",
    "        total_loss += loss.item() * data_loader.batch_size\n",
    "        train_bar.set_description('Train Epoch: [{}/{}], lr: {:.6f}, Loss: {:.4f}'.format(epoch, args.epochs, optimizer.param_groups[0]['lr'], total_loss / total_num))\n",
    "\n",
    "    return total_loss / total_num\n",
    "\n",
    "# lr scheduler for training\n",
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
    "    lr = args.lr\n",
    "    if args.cos:  # cosine lr schedule\n",
    "        lr *= 0.5 * (1. + math.cos(math.pi * epoch / args.epochs))\n",
    "    else:  # stepwise lr schedule\n",
    "        for milestone in args.schedule:\n",
    "            lr *= 0.1 if epoch >= milestone else 1.\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "cellView": "both",
    "id": "RI1Y8bSImD7N"
   },
   "outputs": [],
   "source": [
    "# test using a knn monitor\n",
    "def test(net, memory_data_loader, test_data_loader, epoch, args):\n",
    "    net.eval()\n",
    "    classes = len(memory_data_loader.dataset.classes)\n",
    "    total_top1, total_top5, total_num, feature_bank = 0.0, 0.0, 0, []\n",
    "    with torch.no_grad():\n",
    "        # generate feature bank\n",
    "        for data, target in tqdm(memory_data_loader, desc='Feature extracting'):\n",
    "            feature = net(data.cuda(non_blocking=True))\n",
    "            feature = F.normalize(feature, dim=1)\n",
    "            feature_bank.append(feature)\n",
    "        # [D, N]\n",
    "        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\n",
    "        # [N]\n",
    "        feature_labels = torch.tensor(memory_data_loader.dataset.targets, device=feature_bank.device)\n",
    "        # loop test data to predict the label by weighted knn search\n",
    "        test_bar = tqdm(test_data_loader)\n",
    "        for data, target in test_bar:\n",
    "            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
    "            feature = net(data)\n",
    "            feature = F.normalize(feature, dim=1)\n",
    "            \n",
    "            pred_labels = knn_predict(feature, feature_bank, feature_labels, classes, args.knn_k, args.knn_t)\n",
    "\n",
    "            total_num += data.size(0)\n",
    "            total_top1 += (pred_labels[:, 0] == target).float().sum().item()\n",
    "            test_bar.set_description('Test Epoch: [{}/{}] Acc@1:{:.2f}%'.format(epoch, args.epochs, total_top1 / total_num * 100))\n",
    "\n",
    "    return total_top1 / total_num * 100\n",
    "\n",
    "# knn monitor as in InstDisc https://arxiv.org/abs/1805.01978\n",
    "# implementation follows http://github.com/zhirongw/lemniscate.pytorch and https://github.com/leftthomas/SimCLR\n",
    "def knn_predict(feature, feature_bank, feature_labels, classes, knn_k, knn_t):\n",
    "    # compute cos similarity between each feature vector and feature bank ---> [B, N]\n",
    "    sim_matrix = torch.mm(feature, feature_bank)\n",
    "    # [B, K]\n",
    "    sim_weight, sim_indices = sim_matrix.topk(k=knn_k, dim=-1)\n",
    "    # [B, K]\n",
    "    sim_labels = torch.gather(feature_labels.expand(feature.size(0), -1), dim=-1, index=sim_indices)\n",
    "    sim_weight = (sim_weight / knn_t).exp()\n",
    "\n",
    "    # counts for each class\n",
    "    one_hot_label = torch.zeros(feature.size(0) * knn_k, classes, device=sim_labels.device)\n",
    "    # [B*K, C]\n",
    "    one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)\n",
    "    # weighted score ---> [B, C]\n",
    "    pred_scores = torch.sum(one_hot_label.view(feature.size(0), -1, classes) * sim_weight.unsqueeze(dim=-1), dim=1)\n",
    "\n",
    "    pred_labels = pred_scores.argsort(dim=-1, descending=True)\n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86lHkiKox3KO"
   },
   "source": [
    "### Creat Encoder Network and Model Info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelBase(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): SplitBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): SplitBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): SplitBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): SplitBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): SplitBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): SplitBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): SplitBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): SplitBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): SplitBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): SplitBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): SplitBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): SplitBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): SplitBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): SplitBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): SplitBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): SplitBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): SplitBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): SplitBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): SplitBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): SplitBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (8): Flatten(start_dim=1, end_dim=-1)\n",
      "    (9): Linear(in_features=512, out_features=128, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = ModelMoCo(\n",
    "        dim=args.moco_dim,\n",
    "        K=args.moco_k,\n",
    "        m=args.moco_m,\n",
    "        T=args.moco_t,\n",
    "        arch=args.arch,\n",
    "        bn_splits=args.bn_splits,\n",
    "        symmetric=args.symmetric,\n",
    "    ).cuda()\n",
    "print(model.encoder_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Start Training and Save the Moco Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [1/500], lr: 0.030000, Loss: 4.0285: 100%|██████████| 34/34 [00:04<00:00,  6.97it/s]\n",
      "Train Epoch: [2/500], lr: 0.029999, Loss: 3.9326: 100%|██████████| 34/34 [00:04<00:00,  8.45it/s]\n",
      "Train Epoch: [3/500], lr: 0.029997, Loss: 3.9165: 100%|██████████| 34/34 [00:04<00:00,  8.16it/s]\n",
      "Train Epoch: [4/500], lr: 0.029995, Loss: 3.8679: 100%|██████████| 34/34 [00:04<00:00,  8.09it/s]\n",
      "Train Epoch: [5/500], lr: 0.029993, Loss: 3.7972: 100%|██████████| 34/34 [00:04<00:00,  8.43it/s]\n",
      "Train Epoch: [6/500], lr: 0.029989, Loss: 3.7154: 100%|██████████| 34/34 [00:04<00:00,  8.44it/s]\n",
      "Train Epoch: [7/500], lr: 0.029985, Loss: 3.6389: 100%|██████████| 34/34 [00:04<00:00,  8.43it/s]\n",
      "Train Epoch: [8/500], lr: 0.029981, Loss: 3.5887: 100%|██████████| 34/34 [00:04<00:00,  8.46it/s]\n",
      "Train Epoch: [9/500], lr: 0.029976, Loss: 3.5462: 100%|██████████| 34/34 [00:04<00:00,  8.16it/s]\n",
      "Train Epoch: [10/500], lr: 0.029970, Loss: 3.4616: 100%|██████████| 34/34 [00:04<00:00,  8.15it/s]\n",
      "Train Epoch: [11/500], lr: 0.029964, Loss: 3.3903: 100%|██████████| 34/34 [00:04<00:00,  8.09it/s]\n",
      "Train Epoch: [12/500], lr: 0.029957, Loss: 3.4053: 100%|██████████| 34/34 [00:04<00:00,  8.47it/s]\n",
      "Train Epoch: [13/500], lr: 0.029950, Loss: 3.3776: 100%|██████████| 34/34 [00:04<00:00,  8.21it/s]\n",
      "Train Epoch: [14/500], lr: 0.029942, Loss: 3.3290: 100%|██████████| 34/34 [00:04<00:00,  8.08it/s]\n",
      "Train Epoch: [15/500], lr: 0.029933, Loss: 3.2638: 100%|██████████| 34/34 [00:04<00:00,  8.09it/s]\n",
      "Train Epoch: [16/500], lr: 0.029924, Loss: 3.2357: 100%|██████████| 34/34 [00:04<00:00,  8.14it/s]\n",
      "Train Epoch: [17/500], lr: 0.029915, Loss: 3.2405: 100%|██████████| 34/34 [00:04<00:00,  8.46it/s]\n",
      "Train Epoch: [18/500], lr: 0.029904, Loss: 3.2175: 100%|██████████| 34/34 [00:04<00:00,  8.40it/s]\n",
      "Train Epoch: [19/500], lr: 0.029893, Loss: 3.1729: 100%|██████████| 34/34 [00:03<00:00,  8.51it/s]\n",
      "Train Epoch: [20/500], lr: 0.029882, Loss: 3.1285: 100%|██████████| 34/34 [00:03<00:00,  8.50it/s]\n",
      "Train Epoch: [21/500], lr: 0.029870, Loss: 3.1090: 100%|██████████| 34/34 [00:04<00:00,  8.44it/s]\n",
      "Train Epoch: [22/500], lr: 0.029857, Loss: 3.1195: 100%|██████████| 34/34 [00:04<00:00,  8.49it/s]\n",
      "Train Epoch: [23/500], lr: 0.029844, Loss: 3.0929: 100%|██████████| 34/34 [00:03<00:00,  8.51it/s]\n",
      "Train Epoch: [24/500], lr: 0.029830, Loss: 3.1073: 100%|██████████| 34/34 [00:04<00:00,  8.46it/s]\n",
      "Train Epoch: [25/500], lr: 0.029815, Loss: 3.0389: 100%|██████████| 34/34 [00:04<00:00,  8.44it/s]\n",
      "Train Epoch: [26/500], lr: 0.029800, Loss: 3.0439: 100%|██████████| 34/34 [00:04<00:00,  8.42it/s]\n",
      "Train Epoch: [27/500], lr: 0.029785, Loss: 2.9971: 100%|██████████| 34/34 [00:04<00:00,  8.43it/s]\n",
      "Train Epoch: [28/500], lr: 0.029768, Loss: 2.9725: 100%|██████████| 34/34 [00:04<00:00,  8.48it/s]\n",
      "Train Epoch: [29/500], lr: 0.029752, Loss: 2.9999: 100%|██████████| 34/34 [00:04<00:00,  8.42it/s]\n",
      "Train Epoch: [30/500], lr: 0.029734, Loss: 3.0481: 100%|██████████| 34/34 [00:04<00:00,  8.41it/s]\n",
      "Train Epoch: [31/500], lr: 0.029716, Loss: 3.0158: 100%|██████████| 34/34 [00:04<00:00,  8.41it/s]\n",
      "Train Epoch: [32/500], lr: 0.029698, Loss: 2.9480: 100%|██████████| 34/34 [00:04<00:00,  8.38it/s]\n",
      "Train Epoch: [33/500], lr: 0.029679, Loss: 2.9596: 100%|██████████| 34/34 [00:04<00:00,  8.40it/s]\n",
      "Train Epoch: [34/500], lr: 0.029659, Loss: 2.9214: 100%|██████████| 34/34 [00:04<00:00,  8.46it/s]\n",
      "Train Epoch: [35/500], lr: 0.029639, Loss: 2.8884: 100%|██████████| 34/34 [00:04<00:00,  8.44it/s]\n",
      "Train Epoch: [36/500], lr: 0.029618, Loss: 2.8770: 100%|██████████| 34/34 [00:04<00:00,  8.29it/s]\n",
      "Train Epoch: [37/500], lr: 0.029596, Loss: 2.8591: 100%|██████████| 34/34 [00:04<00:00,  8.14it/s]\n",
      "Train Epoch: [38/500], lr: 0.029574, Loss: 2.8456: 100%|██████████| 34/34 [00:04<00:00,  8.38it/s]\n",
      "Train Epoch: [39/500], lr: 0.029552, Loss: 2.8668: 100%|██████████| 34/34 [00:04<00:00,  8.46it/s]\n",
      "Train Epoch: [40/500], lr: 0.029529, Loss: 2.8873: 100%|██████████| 34/34 [00:04<00:00,  8.30it/s]\n",
      "Train Epoch: [41/500], lr: 0.029505, Loss: 2.8001: 100%|██████████| 34/34 [00:04<00:00,  8.49it/s]\n",
      "Train Epoch: [42/500], lr: 0.029481, Loss: 2.8302: 100%|██████████| 34/34 [00:04<00:00,  8.44it/s]\n",
      "Train Epoch: [43/500], lr: 0.029456, Loss: 2.8324: 100%|██████████| 34/34 [00:03<00:00,  8.51it/s]\n",
      "Train Epoch: [44/500], lr: 0.029430, Loss: 2.8212: 100%|██████████| 34/34 [00:04<00:00,  8.41it/s]\n",
      "Train Epoch: [45/500], lr: 0.029404, Loss: 2.8128: 100%|██████████| 34/34 [00:04<00:00,  8.14it/s]\n",
      "Train Epoch: [46/500], lr: 0.029378, Loss: 2.8138: 100%|██████████| 34/34 [00:04<00:00,  8.18it/s]\n",
      "Train Epoch: [47/500], lr: 0.029351, Loss: 2.7967: 100%|██████████| 34/34 [00:04<00:00,  8.49it/s]\n",
      "Train Epoch: [48/500], lr: 0.029323, Loss: 2.7988: 100%|██████████| 34/34 [00:04<00:00,  8.40it/s]\n",
      "Train Epoch: [49/500], lr: 0.029295, Loss: 2.7765: 100%|██████████| 34/34 [00:04<00:00,  8.43it/s]\n",
      "Train Epoch: [50/500], lr: 0.029266, Loss: 2.7285: 100%|██████████| 34/34 [00:04<00:00,  8.38it/s]\n",
      "Train Epoch: [51/500], lr: 0.029236, Loss: 2.7828: 100%|██████████| 34/34 [00:04<00:00,  8.39it/s]\n",
      "Train Epoch: [52/500], lr: 0.029206, Loss: 2.7430: 100%|██████████| 34/34 [00:04<00:00,  8.39it/s]\n",
      "Train Epoch: [53/500], lr: 0.029176, Loss: 2.7341: 100%|██████████| 34/34 [00:04<00:00,  8.24it/s]\n",
      "Train Epoch: [54/500], lr: 0.029145, Loss: 2.7449: 100%|██████████| 34/34 [00:04<00:00,  8.45it/s]\n",
      "Train Epoch: [55/500], lr: 0.029113, Loss: 2.6866: 100%|██████████| 34/34 [00:03<00:00,  8.50it/s]\n",
      "Train Epoch: [56/500], lr: 0.029081, Loss: 2.7444: 100%|██████████| 34/34 [00:04<00:00,  8.50it/s]\n",
      "Train Epoch: [57/500], lr: 0.029048, Loss: 2.6822: 100%|██████████| 34/34 [00:04<00:00,  8.48it/s]\n",
      "Train Epoch: [58/500], lr: 0.029015, Loss: 2.6394: 100%|██████████| 34/34 [00:03<00:00,  8.53it/s]\n",
      "Train Epoch: [59/500], lr: 0.028981, Loss: 2.7072: 100%|██████████| 34/34 [00:03<00:00,  8.50it/s]\n",
      "Train Epoch: [60/500], lr: 0.028947, Loss: 2.7279: 100%|██████████| 34/34 [00:03<00:00,  8.53it/s]\n",
      "Train Epoch: [61/500], lr: 0.028912, Loss: 2.6461: 100%|██████████| 34/34 [00:03<00:00,  8.51it/s]\n",
      "Train Epoch: [62/500], lr: 0.028876, Loss: 2.6674: 100%|██████████| 34/34 [00:04<00:00,  8.40it/s]\n",
      "Train Epoch: [63/500], lr: 0.028840, Loss: 2.6745: 100%|██████████| 34/34 [00:03<00:00,  8.55it/s]\n",
      "Train Epoch: [64/500], lr: 0.028803, Loss: 2.6506: 100%|██████████| 34/34 [00:03<00:00,  8.54it/s]\n",
      "Train Epoch: [65/500], lr: 0.028766, Loss: 2.6189: 100%|██████████| 34/34 [00:03<00:00,  8.50it/s]\n",
      "Train Epoch: [66/500], lr: 0.028729, Loss: 2.6044: 100%|██████████| 34/34 [00:04<00:00,  8.37it/s]\n",
      "Train Epoch: [67/500], lr: 0.028690, Loss: 2.6523: 100%|██████████| 34/34 [00:04<00:00,  8.48it/s]\n",
      "Train Epoch: [68/500], lr: 0.028652, Loss: 2.6199: 100%|██████████| 34/34 [00:04<00:00,  8.45it/s]\n",
      "Train Epoch: [69/500], lr: 0.028612, Loss: 2.6219: 100%|██████████| 34/34 [00:04<00:00,  8.43it/s]\n",
      "Train Epoch: [70/500], lr: 0.028572, Loss: 2.6026: 100%|██████████| 34/34 [00:04<00:00,  8.43it/s]\n",
      "Train Epoch: [71/500], lr: 0.028532, Loss: 2.6190: 100%|██████████| 34/34 [00:04<00:00,  8.40it/s]\n",
      "Train Epoch: [72/500], lr: 0.028491, Loss: 2.6415: 100%|██████████| 34/34 [00:04<00:00,  8.46it/s]\n",
      "Train Epoch: [73/500], lr: 0.028450, Loss: 2.6053: 100%|██████████| 34/34 [00:04<00:00,  8.46it/s]\n",
      "Train Epoch: [74/500], lr: 0.028408, Loss: 2.5778: 100%|██████████| 34/34 [00:04<00:00,  8.50it/s]\n",
      "Train Epoch: [75/500], lr: 0.028365, Loss: 2.5754: 100%|██████████| 34/34 [00:04<00:00,  8.42it/s]\n",
      "Train Epoch: [76/500], lr: 0.028322, Loss: 2.5821: 100%|██████████| 34/34 [00:04<00:00,  8.47it/s]\n",
      "Train Epoch: [77/500], lr: 0.028278, Loss: 2.5698: 100%|██████████| 34/34 [00:04<00:00,  8.48it/s]\n",
      "Train Epoch: [78/500], lr: 0.028234, Loss: 2.5819: 100%|██████████| 34/34 [00:04<00:00,  8.46it/s]\n",
      "Train Epoch: [79/500], lr: 0.028190, Loss: 2.5656: 100%|██████████| 34/34 [00:04<00:00,  8.40it/s]\n",
      "Train Epoch: [80/500], lr: 0.028145, Loss: 2.5309: 100%|██████████| 34/34 [00:04<00:00,  8.47it/s]\n",
      "Train Epoch: [81/500], lr: 0.028099, Loss: 2.5435: 100%|██████████| 34/34 [00:03<00:00,  8.51it/s]\n",
      "Train Epoch: [82/500], lr: 0.028053, Loss: 2.5405: 100%|██████████| 34/34 [00:04<00:00,  8.46it/s]\n",
      "Train Epoch: [83/500], lr: 0.028006, Loss: 2.5174: 100%|██████████| 34/34 [00:03<00:00,  8.50it/s]\n",
      "Train Epoch: [84/500], lr: 0.027959, Loss: 2.5419: 100%|██████████| 34/34 [00:03<00:00,  8.71it/s]\n",
      "Train Epoch: [85/500], lr: 0.027911, Loss: 2.5315: 100%|██████████| 34/34 [00:03<00:00,  8.55it/s]\n",
      "Train Epoch: [86/500], lr: 0.027863, Loss: 2.5178: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [87/500], lr: 0.027814, Loss: 2.5078: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [88/500], lr: 0.027765, Loss: 2.5358: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [89/500], lr: 0.027715, Loss: 2.5355: 100%|██████████| 34/34 [00:03<00:00,  8.72it/s]\n",
      "Train Epoch: [90/500], lr: 0.027665, Loss: 2.4924: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [91/500], lr: 0.027614, Loss: 2.5103: 100%|██████████| 34/34 [00:03<00:00,  8.70it/s]\n",
      "Train Epoch: [92/500], lr: 0.027563, Loss: 2.4898: 100%|██████████| 34/34 [00:03<00:00,  8.70it/s]\n",
      "Train Epoch: [93/500], lr: 0.027511, Loss: 2.4840: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [94/500], lr: 0.027459, Loss: 2.4633: 100%|██████████| 34/34 [00:03<00:00,  8.61it/s]\n",
      "Train Epoch: [95/500], lr: 0.027406, Loss: 2.4981: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [96/500], lr: 0.027353, Loss: 2.4920: 100%|██████████| 34/34 [00:03<00:00,  8.60it/s]\n",
      "Train Epoch: [97/500], lr: 0.027299, Loss: 2.4920: 100%|██████████| 34/34 [00:03<00:00,  8.66it/s]\n",
      "Train Epoch: [98/500], lr: 0.027245, Loss: 2.4894: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [99/500], lr: 0.027190, Loss: 2.4312: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [100/500], lr: 0.027135, Loss: 2.4625: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [101/500], lr: 0.027080, Loss: 2.4493: 100%|██████████| 34/34 [00:03<00:00,  8.71it/s]\n",
      "Train Epoch: [102/500], lr: 0.027024, Loss: 2.4564: 100%|██████████| 34/34 [00:03<00:00,  8.71it/s]\n",
      "Train Epoch: [103/500], lr: 0.026967, Loss: 2.4418: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [104/500], lr: 0.026910, Loss: 2.4473: 100%|██████████| 34/34 [00:03<00:00,  8.57it/s]\n",
      "Train Epoch: [105/500], lr: 0.026852, Loss: 2.3919: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [106/500], lr: 0.026794, Loss: 2.3913: 100%|██████████| 34/34 [00:03<00:00,  8.58it/s]\n",
      "Train Epoch: [107/500], lr: 0.026736, Loss: 2.4176: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [108/500], lr: 0.026677, Loss: 2.4565: 100%|██████████| 34/34 [00:03<00:00,  8.61it/s]\n",
      "Train Epoch: [109/500], lr: 0.026618, Loss: 2.4002: 100%|██████████| 34/34 [00:03<00:00,  8.70it/s]\n",
      "Train Epoch: [110/500], lr: 0.026558, Loss: 2.3812: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [111/500], lr: 0.026497, Loss: 2.4167: 100%|██████████| 34/34 [00:03<00:00,  8.77it/s]\n",
      "Train Epoch: [112/500], lr: 0.026437, Loss: 2.4294: 100%|██████████| 34/34 [00:03<00:00,  8.73it/s]\n",
      "Train Epoch: [113/500], lr: 0.026375, Loss: 2.4212: 100%|██████████| 34/34 [00:03<00:00,  8.75it/s]\n",
      "Train Epoch: [114/500], lr: 0.026314, Loss: 2.4176: 100%|██████████| 34/34 [00:03<00:00,  8.76it/s]\n",
      "Train Epoch: [115/500], lr: 0.026252, Loss: 2.4114: 100%|██████████| 34/34 [00:03<00:00,  8.74it/s]\n",
      "Train Epoch: [116/500], lr: 0.026189, Loss: 2.3974: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [117/500], lr: 0.026126, Loss: 2.4089: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [118/500], lr: 0.026063, Loss: 2.4099: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [119/500], lr: 0.025999, Loss: 2.3737: 100%|██████████| 34/34 [00:03<00:00,  8.66it/s]\n",
      "Train Epoch: [120/500], lr: 0.025935, Loss: 2.3385: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [121/500], lr: 0.025870, Loss: 2.3759: 100%|██████████| 34/34 [00:03<00:00,  8.57it/s]\n",
      "Train Epoch: [122/500], lr: 0.025805, Loss: 2.3538: 100%|██████████| 34/34 [00:03<00:00,  8.61it/s]\n",
      "Train Epoch: [123/500], lr: 0.025739, Loss: 2.3605: 100%|██████████| 34/34 [00:03<00:00,  8.60it/s]\n",
      "Train Epoch: [124/500], lr: 0.025673, Loss: 2.3783: 100%|██████████| 34/34 [00:04<00:00,  8.32it/s]\n",
      "Train Epoch: [125/500], lr: 0.025607, Loss: 2.4046: 100%|██████████| 34/34 [00:04<00:00,  8.31it/s]\n",
      "Train Epoch: [126/500], lr: 0.025540, Loss: 2.3563: 100%|██████████| 34/34 [00:03<00:00,  8.57it/s]\n",
      "Train Epoch: [127/500], lr: 0.025472, Loss: 2.3202: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [128/500], lr: 0.025405, Loss: 2.3477: 100%|██████████| 34/34 [00:03<00:00,  8.73it/s]\n",
      "Train Epoch: [129/500], lr: 0.025337, Loss: 2.3536: 100%|██████████| 34/34 [00:03<00:00,  8.71it/s]\n",
      "Train Epoch: [130/500], lr: 0.025268, Loss: 2.3401: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [131/500], lr: 0.025199, Loss: 2.3531: 100%|██████████| 34/34 [00:03<00:00,  8.71it/s]\n",
      "Train Epoch: [132/500], lr: 0.025130, Loss: 2.3440: 100%|██████████| 34/34 [00:03<00:00,  8.70it/s]\n",
      "Train Epoch: [133/500], lr: 0.025060, Loss: 2.3559: 100%|██████████| 34/34 [00:03<00:00,  8.62it/s]\n",
      "Train Epoch: [134/500], lr: 0.024990, Loss: 2.3166: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [135/500], lr: 0.024920, Loss: 2.3246: 100%|██████████| 34/34 [00:03<00:00,  8.58it/s]\n",
      "Train Epoch: [136/500], lr: 0.024849, Loss: 2.3068: 100%|██████████| 34/34 [00:03<00:00,  8.56it/s]\n",
      "Train Epoch: [137/500], lr: 0.024778, Loss: 2.2976: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [138/500], lr: 0.024706, Loss: 2.3328: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [139/500], lr: 0.024634, Loss: 2.3330: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [140/500], lr: 0.024561, Loss: 2.2801: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [141/500], lr: 0.024489, Loss: 2.2727: 100%|██████████| 34/34 [00:03<00:00,  8.70it/s]\n",
      "Train Epoch: [142/500], lr: 0.024415, Loss: 2.2583: 100%|██████████| 34/34 [00:03<00:00,  8.61it/s]\n",
      "Train Epoch: [143/500], lr: 0.024342, Loss: 2.2927: 100%|██████████| 34/34 [00:03<00:00,  8.66it/s]\n",
      "Train Epoch: [144/500], lr: 0.024268, Loss: 2.2583: 100%|██████████| 34/34 [00:04<00:00,  8.38it/s]\n",
      "Train Epoch: [145/500], lr: 0.024194, Loss: 2.2546: 100%|██████████| 34/34 [00:04<00:00,  8.42it/s]\n",
      "Train Epoch: [146/500], lr: 0.024119, Loss: 2.2938: 100%|██████████| 34/34 [00:03<00:00,  8.57it/s]\n",
      "Train Epoch: [147/500], lr: 0.024044, Loss: 2.2926: 100%|██████████| 34/34 [00:03<00:00,  8.61it/s]\n",
      "Train Epoch: [148/500], lr: 0.023969, Loss: 2.2837: 100%|██████████| 34/34 [00:04<00:00,  8.49it/s]\n",
      "Train Epoch: [149/500], lr: 0.023893, Loss: 2.2953: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [150/500], lr: 0.023817, Loss: 2.2843: 100%|██████████| 34/34 [00:03<00:00,  8.72it/s]\n",
      "Train Epoch: [151/500], lr: 0.023740, Loss: 2.2526: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [152/500], lr: 0.023664, Loss: 2.2249: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [153/500], lr: 0.023586, Loss: 2.2434: 100%|██████████| 34/34 [00:03<00:00,  8.62it/s]\n",
      "Train Epoch: [154/500], lr: 0.023509, Loss: 2.2790: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [155/500], lr: 0.023431, Loss: 2.2396: 100%|██████████| 34/34 [00:03<00:00,  8.52it/s]\n",
      "Train Epoch: [156/500], lr: 0.023353, Loss: 2.1798: 100%|██████████| 34/34 [00:03<00:00,  8.60it/s]\n",
      "Train Epoch: [157/500], lr: 0.023275, Loss: 2.2170: 100%|██████████| 34/34 [00:03<00:00,  8.62it/s]\n",
      "Train Epoch: [158/500], lr: 0.023196, Loss: 2.2212: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [159/500], lr: 0.023117, Loss: 2.2278: 100%|██████████| 34/34 [00:03<00:00,  8.66it/s]\n",
      "Train Epoch: [160/500], lr: 0.023037, Loss: 2.2195: 100%|██████████| 34/34 [00:03<00:00,  8.66it/s]\n",
      "Train Epoch: [161/500], lr: 0.022958, Loss: 2.2109: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [162/500], lr: 0.022878, Loss: 2.2137: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [163/500], lr: 0.022797, Loss: 2.1728: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [164/500], lr: 0.022717, Loss: 2.2086: 100%|██████████| 34/34 [00:03<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [165/500], lr: 0.022636, Loss: 2.2189: 100%|██████████| 34/34 [00:04<00:00,  8.38it/s]\n",
      "Train Epoch: [166/500], lr: 0.022554, Loss: 2.2042: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [167/500], lr: 0.022473, Loss: 2.2029: 100%|██████████| 34/34 [00:03<00:00,  8.66it/s]\n",
      "Train Epoch: [168/500], lr: 0.022391, Loss: 2.1799: 100%|██████████| 34/34 [00:04<00:00,  8.45it/s]\n",
      "Train Epoch: [169/500], lr: 0.022309, Loss: 2.2020: 100%|██████████| 34/34 [00:04<00:00,  8.34it/s]\n",
      "Train Epoch: [170/500], lr: 0.022226, Loss: 2.1812: 100%|██████████| 34/34 [00:03<00:00,  8.72it/s]\n",
      "Train Epoch: [171/500], lr: 0.022144, Loss: 2.1941: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [172/500], lr: 0.022061, Loss: 2.2217: 100%|██████████| 34/34 [00:03<00:00,  8.59it/s]\n",
      "Train Epoch: [173/500], lr: 0.021977, Loss: 2.2330: 100%|██████████| 34/34 [00:03<00:00,  8.76it/s]\n",
      "Train Epoch: [174/500], lr: 0.021894, Loss: 2.1968: 100%|██████████| 34/34 [00:03<00:00,  8.72it/s]\n",
      "Train Epoch: [175/500], lr: 0.021810, Loss: 2.1967: 100%|██████████| 34/34 [00:03<00:00,  8.72it/s]\n",
      "Train Epoch: [176/500], lr: 0.021726, Loss: 2.1931: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [177/500], lr: 0.021641, Loss: 2.1950: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [178/500], lr: 0.021557, Loss: 2.1861: 100%|██████████| 34/34 [00:03<00:00,  8.72it/s]\n",
      "Train Epoch: [179/500], lr: 0.021472, Loss: 2.2302: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [180/500], lr: 0.021387, Loss: 2.1640: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [181/500], lr: 0.021301, Loss: 2.1675: 100%|██████████| 34/34 [00:03<00:00,  8.66it/s]\n",
      "Train Epoch: [182/500], lr: 0.021216, Loss: 2.1569: 100%|██████████| 34/34 [00:03<00:00,  8.73it/s]\n",
      "Train Epoch: [183/500], lr: 0.021130, Loss: 2.2098: 100%|██████████| 34/34 [00:04<00:00,  8.50it/s]\n",
      "Train Epoch: [184/500], lr: 0.021044, Loss: 2.1747: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [185/500], lr: 0.020957, Loss: 2.1571: 100%|██████████| 34/34 [00:04<00:00,  8.40it/s]\n",
      "Train Epoch: [186/500], lr: 0.020871, Loss: 2.1702: 100%|██████████| 34/34 [00:04<00:00,  8.26it/s]\n",
      "Train Epoch: [187/500], lr: 0.020784, Loss: 2.1798: 100%|██████████| 34/34 [00:04<00:00,  8.34it/s]\n",
      "Train Epoch: [188/500], lr: 0.020697, Loss: 2.1401: 100%|██████████| 34/34 [00:04<00:00,  8.28it/s]\n",
      "Train Epoch: [189/500], lr: 0.020609, Loss: 2.1318: 100%|██████████| 34/34 [00:04<00:00,  8.41it/s]\n",
      "Train Epoch: [190/500], lr: 0.020522, Loss: 2.1618: 100%|██████████| 34/34 [00:03<00:00,  8.70it/s]\n",
      "Train Epoch: [191/500], lr: 0.020434, Loss: 2.1533: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [192/500], lr: 0.020346, Loss: 2.1612: 100%|██████████| 34/34 [00:04<00:00,  8.49it/s]\n",
      "Train Epoch: [193/500], lr: 0.020258, Loss: 2.1254: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [194/500], lr: 0.020170, Loss: 2.1293: 100%|██████████| 34/34 [00:03<00:00,  8.71it/s]\n",
      "Train Epoch: [195/500], lr: 0.020081, Loss: 2.1047: 100%|██████████| 34/34 [00:03<00:00,  8.62it/s]\n",
      "Train Epoch: [196/500], lr: 0.019992, Loss: 2.1436: 100%|██████████| 34/34 [00:03<00:00,  8.59it/s]\n",
      "Train Epoch: [197/500], lr: 0.019903, Loss: 2.1091: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [198/500], lr: 0.019814, Loss: 2.0887: 100%|██████████| 34/34 [00:03<00:00,  8.73it/s]\n",
      "Train Epoch: [199/500], lr: 0.019725, Loss: 2.1127: 100%|██████████| 34/34 [00:03<00:00,  8.57it/s]\n",
      "Train Epoch: [200/500], lr: 0.019635, Loss: 2.0799: 100%|██████████| 34/34 [00:03<00:00,  8.55it/s]\n",
      "Train Epoch: [201/500], lr: 0.019546, Loss: 2.1313: 100%|██████████| 34/34 [00:03<00:00,  8.62it/s]\n",
      "Train Epoch: [202/500], lr: 0.019456, Loss: 2.1080: 100%|██████████| 34/34 [00:03<00:00,  8.62it/s]\n",
      "Train Epoch: [203/500], lr: 0.019366, Loss: 2.1079: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [204/500], lr: 0.019275, Loss: 2.0749: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [205/500], lr: 0.019185, Loss: 2.0768: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [206/500], lr: 0.019094, Loss: 2.1093: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [207/500], lr: 0.019004, Loss: 2.0883: 100%|██████████| 34/34 [00:03<00:00,  8.74it/s]\n",
      "Train Epoch: [208/500], lr: 0.018913, Loss: 2.1110: 100%|██████████| 34/34 [00:03<00:00,  8.56it/s]\n",
      "Train Epoch: [209/500], lr: 0.018822, Loss: 2.0894: 100%|██████████| 34/34 [00:03<00:00,  8.58it/s]\n",
      "Train Epoch: [210/500], lr: 0.018730, Loss: 2.0726: 100%|██████████| 34/34 [00:03<00:00,  8.61it/s]\n",
      "Train Epoch: [211/500], lr: 0.018639, Loss: 2.0598: 100%|██████████| 34/34 [00:03<00:00,  8.70it/s]\n",
      "Train Epoch: [212/500], lr: 0.018547, Loss: 2.0864: 100%|██████████| 34/34 [00:03<00:00,  8.57it/s]\n",
      "Train Epoch: [213/500], lr: 0.018456, Loss: 2.0414: 100%|██████████| 34/34 [00:03<00:00,  8.70it/s]\n",
      "Train Epoch: [214/500], lr: 0.018364, Loss: 2.0960: 100%|██████████| 34/34 [00:03<00:00,  8.66it/s]\n",
      "Train Epoch: [215/500], lr: 0.018272, Loss: 2.0547: 100%|██████████| 34/34 [00:03<00:00,  8.59it/s]\n",
      "Train Epoch: [216/500], lr: 0.018180, Loss: 2.0804: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [217/500], lr: 0.018088, Loss: 2.0805: 100%|██████████| 34/34 [00:03<00:00,  8.55it/s]\n",
      "Train Epoch: [218/500], lr: 0.017996, Loss: 2.0317: 100%|██████████| 34/34 [00:03<00:00,  8.52it/s]\n",
      "Train Epoch: [219/500], lr: 0.017903, Loss: 2.0643: 100%|██████████| 34/34 [00:03<00:00,  8.72it/s]\n",
      "Train Epoch: [220/500], lr: 0.017811, Loss: 2.0482: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [221/500], lr: 0.017718, Loss: 2.0653: 100%|██████████| 34/34 [00:03<00:00,  8.75it/s]\n",
      "Train Epoch: [222/500], lr: 0.017625, Loss: 2.0399: 100%|██████████| 34/34 [00:03<00:00,  8.60it/s]\n",
      "Train Epoch: [223/500], lr: 0.017533, Loss: 2.0653: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [224/500], lr: 0.017440, Loss: 2.0475: 100%|██████████| 34/34 [00:03<00:00,  8.66it/s]\n",
      "Train Epoch: [225/500], lr: 0.017347, Loss: 1.9814: 100%|██████████| 34/34 [00:03<00:00,  8.56it/s]\n",
      "Train Epoch: [226/500], lr: 0.017253, Loss: 2.0226: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [227/500], lr: 0.017160, Loss: 2.0354: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [228/500], lr: 0.017067, Loss: 2.0577: 100%|██████████| 34/34 [00:03<00:00,  8.72it/s]\n",
      "Train Epoch: [229/500], lr: 0.016973, Loss: 2.0328: 100%|██████████| 34/34 [00:03<00:00,  8.70it/s]\n",
      "Train Epoch: [230/500], lr: 0.016880, Loss: 2.0494: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [231/500], lr: 0.016786, Loss: 2.0480: 100%|██████████| 34/34 [00:03<00:00,  8.62it/s]\n",
      "Train Epoch: [232/500], lr: 0.016693, Loss: 2.0042: 100%|██████████| 34/34 [00:03<00:00,  8.74it/s]\n",
      "Train Epoch: [233/500], lr: 0.016599, Loss: 1.9861: 100%|██████████| 34/34 [00:03<00:00,  8.76it/s]\n",
      "Train Epoch: [234/500], lr: 0.016505, Loss: 2.0390: 100%|██████████| 34/34 [00:03<00:00,  8.71it/s]\n",
      "Train Epoch: [235/500], lr: 0.016412, Loss: 2.0182: 100%|██████████| 34/34 [00:03<00:00,  8.73it/s]\n",
      "Train Epoch: [236/500], lr: 0.016318, Loss: 2.0377: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [237/500], lr: 0.016224, Loss: 2.0407: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [238/500], lr: 0.016130, Loss: 2.0188: 100%|██████████| 34/34 [00:03<00:00,  8.62it/s]\n",
      "Train Epoch: [239/500], lr: 0.016036, Loss: 2.0084: 100%|██████████| 34/34 [00:03<00:00,  8.78it/s]\n",
      "Train Epoch: [240/500], lr: 0.015942, Loss: 2.0070: 100%|██████████| 34/34 [00:03<00:00,  8.75it/s]\n",
      "Train Epoch: [241/500], lr: 0.015848, Loss: 1.9933: 100%|██████████| 34/34 [00:03<00:00,  8.73it/s]\n",
      "Train Epoch: [242/500], lr: 0.015754, Loss: 1.9937: 100%|██████████| 34/34 [00:03<00:00,  8.70it/s]\n",
      "Train Epoch: [243/500], lr: 0.015660, Loss: 1.9832: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [244/500], lr: 0.015565, Loss: 1.9879: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [245/500], lr: 0.015471, Loss: 1.9895: 100%|██████████| 34/34 [00:03<00:00,  8.56it/s]\n",
      "Train Epoch: [246/500], lr: 0.015377, Loss: 2.0041: 100%|██████████| 34/34 [00:04<00:00,  8.42it/s]\n",
      "Train Epoch: [247/500], lr: 0.015283, Loss: 1.9883: 100%|██████████| 34/34 [00:03<00:00,  8.76it/s]\n",
      "Train Epoch: [248/500], lr: 0.015188, Loss: 1.9611: 100%|██████████| 34/34 [00:03<00:00,  8.75it/s]\n",
      "Train Epoch: [249/500], lr: 0.015094, Loss: 1.9490: 100%|██████████| 34/34 [00:03<00:00,  8.74it/s]\n",
      "Train Epoch: [250/500], lr: 0.015000, Loss: 1.9746: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [251/500], lr: 0.014906, Loss: 1.9752: 100%|██████████| 34/34 [00:03<00:00,  8.72it/s]\n",
      "Train Epoch: [252/500], lr: 0.014812, Loss: 1.9778: 100%|██████████| 34/34 [00:03<00:00,  8.72it/s]\n",
      "Train Epoch: [253/500], lr: 0.014717, Loss: 1.9594: 100%|██████████| 34/34 [00:03<00:00,  8.73it/s]\n",
      "Train Epoch: [254/500], lr: 0.014623, Loss: 1.9412: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [255/500], lr: 0.014529, Loss: 1.9525: 100%|██████████| 34/34 [00:04<00:00,  8.28it/s]\n",
      "Train Epoch: [256/500], lr: 0.014435, Loss: 1.9405: 100%|██████████| 34/34 [00:04<00:00,  8.34it/s]\n",
      "Train Epoch: [257/500], lr: 0.014340, Loss: 1.9532: 100%|██████████| 34/34 [00:04<00:00,  8.17it/s]\n",
      "Train Epoch: [258/500], lr: 0.014246, Loss: 1.9669: 100%|██████████| 34/34 [00:04<00:00,  8.34it/s]\n",
      "Train Epoch: [259/500], lr: 0.014152, Loss: 1.9747: 100%|██████████| 34/34 [00:04<00:00,  8.33it/s]\n",
      "Train Epoch: [260/500], lr: 0.014058, Loss: 1.9448: 100%|██████████| 34/34 [00:04<00:00,  8.29it/s]\n",
      "Train Epoch: [261/500], lr: 0.013964, Loss: 1.9796: 100%|██████████| 34/34 [00:04<00:00,  8.32it/s]\n",
      "Train Epoch: [262/500], lr: 0.013870, Loss: 1.9520: 100%|██████████| 34/34 [00:04<00:00,  8.20it/s]\n",
      "Train Epoch: [263/500], lr: 0.013776, Loss: 1.9600: 100%|██████████| 34/34 [00:04<00:00,  8.25it/s]\n",
      "Train Epoch: [264/500], lr: 0.013682, Loss: 1.9182: 100%|██████████| 34/34 [00:04<00:00,  8.27it/s]\n",
      "Train Epoch: [265/500], lr: 0.013588, Loss: 1.9483: 100%|██████████| 34/34 [00:04<00:00,  8.23it/s]\n",
      "Train Epoch: [266/500], lr: 0.013495, Loss: 1.8785: 100%|██████████| 34/34 [00:04<00:00,  8.19it/s]\n",
      "Train Epoch: [267/500], lr: 0.013401, Loss: 1.9284: 100%|██████████| 34/34 [00:04<00:00,  8.28it/s]\n",
      "Train Epoch: [268/500], lr: 0.013307, Loss: 1.9055: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [269/500], lr: 0.013214, Loss: 1.9182: 100%|██████████| 34/34 [00:03<00:00,  8.60it/s]\n",
      "Train Epoch: [270/500], lr: 0.013120, Loss: 1.9210: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [271/500], lr: 0.013027, Loss: 1.9530: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [272/500], lr: 0.012933, Loss: 1.9063: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [273/500], lr: 0.012840, Loss: 1.9281: 100%|██████████| 34/34 [00:03<00:00,  8.60it/s]\n",
      "Train Epoch: [274/500], lr: 0.012747, Loss: 1.9269: 100%|██████████| 34/34 [00:04<00:00,  8.39it/s]\n",
      "Train Epoch: [275/500], lr: 0.012653, Loss: 1.9022: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [276/500], lr: 0.012560, Loss: 1.9251: 100%|██████████| 34/34 [00:03<00:00,  8.54it/s]\n",
      "Train Epoch: [277/500], lr: 0.012467, Loss: 1.9068: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [278/500], lr: 0.012375, Loss: 1.9268: 100%|██████████| 34/34 [00:03<00:00,  8.70it/s]\n",
      "Train Epoch: [279/500], lr: 0.012282, Loss: 1.9134: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [280/500], lr: 0.012189, Loss: 1.8522: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [281/500], lr: 0.012097, Loss: 1.8978: 100%|██████████| 34/34 [00:03<00:00,  8.73it/s]\n",
      "Train Epoch: [282/500], lr: 0.012004, Loss: 1.8937: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [283/500], lr: 0.011912, Loss: 1.9170: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [284/500], lr: 0.011820, Loss: 1.9151: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [285/500], lr: 0.011728, Loss: 1.8684: 100%|██████████| 34/34 [00:03<00:00,  8.66it/s]\n",
      "Train Epoch: [286/500], lr: 0.011636, Loss: 1.8862: 100%|██████████| 34/34 [00:03<00:00,  8.62it/s]\n",
      "Train Epoch: [287/500], lr: 0.011544, Loss: 1.8689: 100%|██████████| 34/34 [00:04<00:00,  8.39it/s]\n",
      "Train Epoch: [288/500], lr: 0.011453, Loss: 1.8807: 100%|██████████| 34/34 [00:04<00:00,  8.38it/s]\n",
      "Train Epoch: [289/500], lr: 0.011361, Loss: 1.8583: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [290/500], lr: 0.011270, Loss: 1.8702: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [291/500], lr: 0.011178, Loss: 1.8723: 100%|██████████| 34/34 [00:04<00:00,  8.35it/s]\n",
      "Train Epoch: [292/500], lr: 0.011087, Loss: 1.8713: 100%|██████████| 34/34 [00:04<00:00,  8.36it/s]\n",
      "Train Epoch: [293/500], lr: 0.010996, Loss: 1.8610: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [294/500], lr: 0.010906, Loss: 1.8501: 100%|██████████| 34/34 [00:03<00:00,  8.53it/s]\n",
      "Train Epoch: [295/500], lr: 0.010815, Loss: 1.8670: 100%|██████████| 34/34 [00:04<00:00,  8.32it/s]\n",
      "Train Epoch: [296/500], lr: 0.010725, Loss: 1.8523: 100%|██████████| 34/34 [00:04<00:00,  8.25it/s]\n",
      "Train Epoch: [297/500], lr: 0.010634, Loss: 1.8671: 100%|██████████| 34/34 [00:04<00:00,  8.30it/s]\n",
      "Train Epoch: [298/500], lr: 0.010544, Loss: 1.8100: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [299/500], lr: 0.010454, Loss: 1.8375: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [300/500], lr: 0.010365, Loss: 1.8476: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [301/500], lr: 0.010275, Loss: 1.8419: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [302/500], lr: 0.010186, Loss: 1.8441: 100%|██████████| 34/34 [00:03<00:00,  8.61it/s]\n",
      "Train Epoch: [303/500], lr: 0.010097, Loss: 1.8499: 100%|██████████| 34/34 [00:04<00:00,  8.45it/s]\n",
      "Train Epoch: [304/500], lr: 0.010008, Loss: 1.8683: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [305/500], lr: 0.009919, Loss: 1.8279: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [306/500], lr: 0.009830, Loss: 1.8321: 100%|██████████| 34/34 [00:03<00:00,  8.61it/s]\n",
      "Train Epoch: [307/500], lr: 0.009742, Loss: 1.8468: 100%|██████████| 34/34 [00:03<00:00,  8.66it/s]\n",
      "Train Epoch: [308/500], lr: 0.009654, Loss: 1.8433: 100%|██████████| 34/34 [00:03<00:00,  8.61it/s]\n",
      "Train Epoch: [309/500], lr: 0.009566, Loss: 1.8695: 100%|██████████| 34/34 [00:03<00:00,  8.66it/s]\n",
      "Train Epoch: [310/500], lr: 0.009478, Loss: 1.7781: 100%|██████████| 34/34 [00:03<00:00,  8.71it/s]\n",
      "Train Epoch: [311/500], lr: 0.009391, Loss: 1.8254: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [312/500], lr: 0.009303, Loss: 1.8300: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [313/500], lr: 0.009216, Loss: 1.8320: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [314/500], lr: 0.009129, Loss: 1.8559: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [315/500], lr: 0.009043, Loss: 1.8110: 100%|██████████| 34/34 [00:03<00:00,  8.66it/s]\n",
      "Train Epoch: [316/500], lr: 0.008956, Loss: 1.7903: 100%|██████████| 34/34 [00:03<00:00,  8.58it/s]\n",
      "Train Epoch: [317/500], lr: 0.008870, Loss: 1.8259: 100%|██████████| 34/34 [00:03<00:00,  8.62it/s]\n",
      "Train Epoch: [318/500], lr: 0.008784, Loss: 1.8087: 100%|██████████| 34/34 [00:03<00:00,  8.60it/s]\n",
      "Train Epoch: [319/500], lr: 0.008699, Loss: 1.7655: 100%|██████████| 34/34 [00:03<00:00,  8.61it/s]\n",
      "Train Epoch: [320/500], lr: 0.008613, Loss: 1.7599: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [321/500], lr: 0.008528, Loss: 1.7714: 100%|██████████| 34/34 [00:03<00:00,  8.73it/s]\n",
      "Train Epoch: [322/500], lr: 0.008443, Loss: 1.7974: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [323/500], lr: 0.008359, Loss: 1.7790: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [324/500], lr: 0.008274, Loss: 1.7950: 100%|██████████| 34/34 [00:03<00:00,  8.54it/s]\n",
      "Train Epoch: [325/500], lr: 0.008190, Loss: 1.7752: 100%|██████████| 34/34 [00:03<00:00,  8.57it/s]\n",
      "Train Epoch: [326/500], lr: 0.008106, Loss: 1.7837: 100%|██████████| 34/34 [00:03<00:00,  8.61it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [327/500], lr: 0.008023, Loss: 1.7579: 100%|██████████| 34/34 [00:03<00:00,  8.57it/s]\n",
      "Train Epoch: [328/500], lr: 0.007939, Loss: 1.7822: 100%|██████████| 34/34 [00:04<00:00,  8.27it/s]\n",
      "Train Epoch: [329/500], lr: 0.007856, Loss: 1.7680: 100%|██████████| 34/34 [00:04<00:00,  8.31it/s]\n",
      "Train Epoch: [330/500], lr: 0.007774, Loss: 1.7642: 100%|██████████| 34/34 [00:04<00:00,  8.37it/s]\n",
      "Train Epoch: [331/500], lr: 0.007691, Loss: 1.7769: 100%|██████████| 34/34 [00:04<00:00,  8.36it/s]\n",
      "Train Epoch: [332/500], lr: 0.007609, Loss: 1.7499: 100%|██████████| 34/34 [00:04<00:00,  8.22it/s]\n",
      "Train Epoch: [333/500], lr: 0.007527, Loss: 1.7487: 100%|██████████| 34/34 [00:04<00:00,  8.31it/s]\n",
      "Train Epoch: [334/500], lr: 0.007446, Loss: 1.7435: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [335/500], lr: 0.007364, Loss: 1.7660: 100%|██████████| 34/34 [00:04<00:00,  8.46it/s]\n",
      "Train Epoch: [336/500], lr: 0.007283, Loss: 1.7666: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [337/500], lr: 0.007203, Loss: 1.7626: 100%|██████████| 34/34 [00:04<00:00,  8.47it/s]\n",
      "Train Epoch: [338/500], lr: 0.007122, Loss: 1.7579: 100%|██████████| 34/34 [00:03<00:00,  8.55it/s]\n",
      "Train Epoch: [339/500], lr: 0.007042, Loss: 1.7247: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [340/500], lr: 0.006963, Loss: 1.7551: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [341/500], lr: 0.006883, Loss: 1.7580: 100%|██████████| 34/34 [00:03<00:00,  8.72it/s]\n",
      "Train Epoch: [342/500], lr: 0.006804, Loss: 1.7347: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [343/500], lr: 0.006725, Loss: 1.7863: 100%|██████████| 34/34 [00:03<00:00,  8.70it/s]\n",
      "Train Epoch: [344/500], lr: 0.006647, Loss: 1.7167: 100%|██████████| 34/34 [00:03<00:00,  8.53it/s]\n",
      "Train Epoch: [345/500], lr: 0.006569, Loss: 1.7201: 100%|██████████| 34/34 [00:03<00:00,  8.54it/s]\n",
      "Train Epoch: [346/500], lr: 0.006491, Loss: 1.7714: 100%|██████████| 34/34 [00:03<00:00,  8.61it/s]\n",
      "Train Epoch: [347/500], lr: 0.006414, Loss: 1.6961: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [348/500], lr: 0.006336, Loss: 1.7372: 100%|██████████| 34/34 [00:04<00:00,  8.26it/s]\n",
      "Train Epoch: [349/500], lr: 0.006260, Loss: 1.6789: 100%|██████████| 34/34 [00:04<00:00,  8.32it/s]\n",
      "Train Epoch: [350/500], lr: 0.006183, Loss: 1.7306: 100%|██████████| 34/34 [00:04<00:00,  8.36it/s]\n",
      "Train Epoch: [351/500], lr: 0.006107, Loss: 1.7251: 100%|██████████| 34/34 [00:04<00:00,  8.43it/s]\n",
      "Train Epoch: [352/500], lr: 0.006031, Loss: 1.7261: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [353/500], lr: 0.005956, Loss: 1.7156: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [354/500], lr: 0.005881, Loss: 1.7088: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [355/500], lr: 0.005806, Loss: 1.6606: 100%|██████████| 34/34 [00:04<00:00,  8.41it/s]\n",
      "Train Epoch: [356/500], lr: 0.005732, Loss: 1.6982: 100%|██████████| 34/34 [00:04<00:00,  8.34it/s]\n",
      "Train Epoch: [357/500], lr: 0.005658, Loss: 1.6834: 100%|██████████| 34/34 [00:04<00:00,  8.33it/s]\n",
      "Train Epoch: [358/500], lr: 0.005585, Loss: 1.6910: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [359/500], lr: 0.005511, Loss: 1.6831: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [360/500], lr: 0.005439, Loss: 1.6824: 100%|██████████| 34/34 [00:04<00:00,  8.36it/s]\n",
      "Train Epoch: [361/500], lr: 0.005366, Loss: 1.6982: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [362/500], lr: 0.005294, Loss: 1.6978: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [363/500], lr: 0.005222, Loss: 1.6824: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [364/500], lr: 0.005151, Loss: 1.6891: 100%|██████████| 34/34 [00:03<00:00,  8.73it/s]\n",
      "Train Epoch: [365/500], lr: 0.005080, Loss: 1.6745: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [366/500], lr: 0.005010, Loss: 1.6619: 100%|██████████| 34/34 [00:03<00:00,  8.66it/s]\n",
      "Train Epoch: [367/500], lr: 0.004940, Loss: 1.6554: 100%|██████████| 34/34 [00:03<00:00,  8.73it/s]\n",
      "Train Epoch: [368/500], lr: 0.004870, Loss: 1.6475: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [369/500], lr: 0.004801, Loss: 1.6672: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [370/500], lr: 0.004732, Loss: 1.6544: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [371/500], lr: 0.004663, Loss: 1.6521: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [372/500], lr: 0.004595, Loss: 1.6590: 100%|██████████| 34/34 [00:04<00:00,  8.48it/s]\n",
      "Train Epoch: [373/500], lr: 0.004528, Loss: 1.6659: 100%|██████████| 34/34 [00:04<00:00,  8.47it/s]\n",
      "Train Epoch: [374/500], lr: 0.004460, Loss: 1.6552: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [375/500], lr: 0.004393, Loss: 1.6513: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [376/500], lr: 0.004327, Loss: 1.6246: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [377/500], lr: 0.004261, Loss: 1.6583: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [378/500], lr: 0.004195, Loss: 1.6474: 100%|██████████| 34/34 [00:03<00:00,  8.66it/s]\n",
      "Train Epoch: [379/500], lr: 0.004130, Loss: 1.6590: 100%|██████████| 34/34 [00:03<00:00,  8.58it/s]\n",
      "Train Epoch: [380/500], lr: 0.004065, Loss: 1.6479: 100%|██████████| 34/34 [00:03<00:00,  8.62it/s]\n",
      "Train Epoch: [381/500], lr: 0.004001, Loss: 1.6552: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [382/500], lr: 0.003937, Loss: 1.6501: 100%|██████████| 34/34 [00:03<00:00,  8.73it/s]\n",
      "Train Epoch: [383/500], lr: 0.003874, Loss: 1.6678: 100%|██████████| 34/34 [00:03<00:00,  8.50it/s]\n",
      "Train Epoch: [384/500], lr: 0.003811, Loss: 1.6432: 100%|██████████| 34/34 [00:03<00:00,  8.60it/s]\n",
      "Train Epoch: [385/500], lr: 0.003748, Loss: 1.6758: 100%|██████████| 34/34 [00:03<00:00,  8.72it/s]\n",
      "Train Epoch: [386/500], lr: 0.003686, Loss: 1.6577: 100%|██████████| 34/34 [00:03<00:00,  8.75it/s]\n",
      "Train Epoch: [387/500], lr: 0.003625, Loss: 1.6235: 100%|██████████| 34/34 [00:03<00:00,  8.76it/s]\n",
      "Train Epoch: [388/500], lr: 0.003563, Loss: 1.6122: 100%|██████████| 34/34 [00:03<00:00,  8.74it/s]\n",
      "Train Epoch: [389/500], lr: 0.003503, Loss: 1.6172: 100%|██████████| 34/34 [00:03<00:00,  8.74it/s]\n",
      "Train Epoch: [390/500], lr: 0.003442, Loss: 1.6671: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [391/500], lr: 0.003382, Loss: 1.5879: 100%|██████████| 34/34 [00:03<00:00,  8.71it/s]\n",
      "Train Epoch: [392/500], lr: 0.003323, Loss: 1.6228: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [393/500], lr: 0.003264, Loss: 1.6124: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [394/500], lr: 0.003206, Loss: 1.6200: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [395/500], lr: 0.003148, Loss: 1.5936: 100%|██████████| 34/34 [00:03<00:00,  8.70it/s]\n",
      "Train Epoch: [396/500], lr: 0.003090, Loss: 1.6274: 100%|██████████| 34/34 [00:03<00:00,  8.73it/s]\n",
      "Train Epoch: [397/500], lr: 0.003033, Loss: 1.6132: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [398/500], lr: 0.002976, Loss: 1.6270: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [399/500], lr: 0.002920, Loss: 1.5935: 100%|██████████| 34/34 [00:03<00:00,  8.74it/s]\n",
      "Train Epoch: [400/500], lr: 0.002865, Loss: 1.5884: 100%|██████████| 34/34 [00:03<00:00,  8.76it/s]\n",
      "Train Epoch: [401/500], lr: 0.002810, Loss: 1.5941: 100%|██████████| 34/34 [00:03<00:00,  8.74it/s]\n",
      "Train Epoch: [402/500], lr: 0.002755, Loss: 1.5903: 100%|██████████| 34/34 [00:03<00:00,  8.60it/s]\n",
      "Train Epoch: [403/500], lr: 0.002701, Loss: 1.5888: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [404/500], lr: 0.002647, Loss: 1.5983: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [405/500], lr: 0.002594, Loss: 1.6061: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [406/500], lr: 0.002541, Loss: 1.5958: 100%|██████████| 34/34 [00:03<00:00,  8.73it/s]\n",
      "Train Epoch: [407/500], lr: 0.002489, Loss: 1.5965: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [408/500], lr: 0.002437, Loss: 1.5873: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [409/500], lr: 0.002386, Loss: 1.5828: 100%|██████████| 34/34 [00:03<00:00,  8.61it/s]\n",
      "Train Epoch: [410/500], lr: 0.002335, Loss: 1.5682: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [411/500], lr: 0.002285, Loss: 1.6098: 100%|██████████| 34/34 [00:03<00:00,  8.53it/s]\n",
      "Train Epoch: [412/500], lr: 0.002235, Loss: 1.5912: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [413/500], lr: 0.002186, Loss: 1.5752: 100%|██████████| 34/34 [00:03<00:00,  8.59it/s]\n",
      "Train Epoch: [414/500], lr: 0.002137, Loss: 1.5983: 100%|██████████| 34/34 [00:03<00:00,  8.72it/s]\n",
      "Train Epoch: [415/500], lr: 0.002089, Loss: 1.5804: 100%|██████████| 34/34 [00:03<00:00,  8.71it/s]\n",
      "Train Epoch: [416/500], lr: 0.002041, Loss: 1.5702: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [417/500], lr: 0.001994, Loss: 1.5867: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [418/500], lr: 0.001947, Loss: 1.5902: 100%|██████████| 34/34 [00:04<00:00,  8.28it/s]\n",
      "Train Epoch: [419/500], lr: 0.001901, Loss: 1.5521: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [420/500], lr: 0.001855, Loss: 1.6001: 100%|██████████| 34/34 [00:03<00:00,  8.58it/s]\n",
      "Train Epoch: [421/500], lr: 0.001810, Loss: 1.5644: 100%|██████████| 34/34 [00:03<00:00,  8.72it/s]\n",
      "Train Epoch: [422/500], lr: 0.001766, Loss: 1.5731: 100%|██████████| 34/34 [00:03<00:00,  8.71it/s]\n",
      "Train Epoch: [423/500], lr: 0.001722, Loss: 1.5593: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [424/500], lr: 0.001678, Loss: 1.5482: 100%|██████████| 34/34 [00:04<00:00,  8.33it/s]\n",
      "Train Epoch: [425/500], lr: 0.001635, Loss: 1.5299: 100%|██████████| 34/34 [00:04<00:00,  8.27it/s]\n",
      "Train Epoch: [426/500], lr: 0.001592, Loss: 1.5569: 100%|██████████| 34/34 [00:04<00:00,  8.45it/s]\n",
      "Train Epoch: [427/500], lr: 0.001550, Loss: 1.5578: 100%|██████████| 34/34 [00:04<00:00,  8.27it/s]\n",
      "Train Epoch: [428/500], lr: 0.001509, Loss: 1.5292: 100%|██████████| 34/34 [00:03<00:00,  8.53it/s]\n",
      "Train Epoch: [429/500], lr: 0.001468, Loss: 1.5309: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [430/500], lr: 0.001428, Loss: 1.5407: 100%|██████████| 34/34 [00:03<00:00,  8.58it/s]\n",
      "Train Epoch: [431/500], lr: 0.001388, Loss: 1.5339: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [432/500], lr: 0.001348, Loss: 1.5456: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [433/500], lr: 0.001310, Loss: 1.5445: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [434/500], lr: 0.001271, Loss: 1.5256: 100%|██████████| 34/34 [00:04<00:00,  8.33it/s]\n",
      "Train Epoch: [435/500], lr: 0.001234, Loss: 1.5558: 100%|██████████| 34/34 [00:04<00:00,  8.26it/s]\n",
      "Train Epoch: [436/500], lr: 0.001197, Loss: 1.5425: 100%|██████████| 34/34 [00:04<00:00,  8.36it/s]\n",
      "Train Epoch: [437/500], lr: 0.001160, Loss: 1.5473: 100%|██████████| 34/34 [00:04<00:00,  8.29it/s]\n",
      "Train Epoch: [438/500], lr: 0.001124, Loss: 1.5164: 100%|██████████| 34/34 [00:04<00:00,  8.32it/s]\n",
      "Train Epoch: [439/500], lr: 0.001088, Loss: 1.5469: 100%|██████████| 34/34 [00:04<00:00,  8.24it/s]\n",
      "Train Epoch: [440/500], lr: 0.001053, Loss: 1.5352: 100%|██████████| 34/34 [00:03<00:00,  8.72it/s]\n",
      "Train Epoch: [441/500], lr: 0.001019, Loss: 1.5261: 100%|██████████| 34/34 [00:03<00:00,  8.56it/s]\n",
      "Train Epoch: [442/500], lr: 0.000985, Loss: 1.5358: 100%|██████████| 34/34 [00:03<00:00,  8.60it/s]\n",
      "Train Epoch: [443/500], lr: 0.000952, Loss: 1.5167: 100%|██████████| 34/34 [00:03<00:00,  8.58it/s]\n",
      "Train Epoch: [444/500], lr: 0.000919, Loss: 1.5660: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [445/500], lr: 0.000887, Loss: 1.5233: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [446/500], lr: 0.000855, Loss: 1.5236: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [447/500], lr: 0.000824, Loss: 1.5403: 100%|██████████| 34/34 [00:03<00:00,  8.66it/s]\n",
      "Train Epoch: [448/500], lr: 0.000794, Loss: 1.5298: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [449/500], lr: 0.000764, Loss: 1.5249: 100%|██████████| 34/34 [00:03<00:00,  8.65it/s]\n",
      "Train Epoch: [450/500], lr: 0.000734, Loss: 1.5188: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [451/500], lr: 0.000705, Loss: 1.5175: 100%|██████████| 34/34 [00:03<00:00,  8.72it/s]\n",
      "Train Epoch: [452/500], lr: 0.000677, Loss: 1.5256: 100%|██████████| 34/34 [00:03<00:00,  8.55it/s]\n",
      "Train Epoch: [453/500], lr: 0.000649, Loss: 1.4925: 100%|██████████| 34/34 [00:03<00:00,  8.61it/s]\n",
      "Train Epoch: [454/500], lr: 0.000622, Loss: 1.5268: 100%|██████████| 34/34 [00:03<00:00,  8.55it/s]\n",
      "Train Epoch: [455/500], lr: 0.000596, Loss: 1.5320: 100%|██████████| 34/34 [00:04<00:00,  8.30it/s]\n",
      "Train Epoch: [456/500], lr: 0.000570, Loss: 1.5095: 100%|██████████| 34/34 [00:04<00:00,  8.36it/s]\n",
      "Train Epoch: [457/500], lr: 0.000544, Loss: 1.5462: 100%|██████████| 34/34 [00:04<00:00,  8.46it/s]\n",
      "Train Epoch: [458/500], lr: 0.000519, Loss: 1.5213: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [459/500], lr: 0.000495, Loss: 1.5329: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [460/500], lr: 0.000471, Loss: 1.5406: 100%|██████████| 34/34 [00:03<00:00,  8.55it/s]\n",
      "Train Epoch: [461/500], lr: 0.000448, Loss: 1.4784: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [462/500], lr: 0.000426, Loss: 1.4980: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [463/500], lr: 0.000404, Loss: 1.5195: 100%|██████████| 34/34 [00:03<00:00,  8.73it/s]\n",
      "Train Epoch: [464/500], lr: 0.000382, Loss: 1.5072: 100%|██████████| 34/34 [00:03<00:00,  8.54it/s]\n",
      "Train Epoch: [465/500], lr: 0.000361, Loss: 1.5323: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [466/500], lr: 0.000341, Loss: 1.5265: 100%|██████████| 34/34 [00:03<00:00,  8.61it/s]\n",
      "Train Epoch: [467/500], lr: 0.000321, Loss: 1.4891: 100%|██████████| 34/34 [00:03<00:00,  8.61it/s]\n",
      "Train Epoch: [468/500], lr: 0.000302, Loss: 1.4898: 100%|██████████| 34/34 [00:03<00:00,  8.72it/s]\n",
      "Train Epoch: [469/500], lr: 0.000284, Loss: 1.5116: 100%|██████████| 34/34 [00:03<00:00,  8.70it/s]\n",
      "Train Epoch: [470/500], lr: 0.000266, Loss: 1.4822: 100%|██████████| 34/34 [00:03<00:00,  8.60it/s]\n",
      "Train Epoch: [471/500], lr: 0.000248, Loss: 1.4742: 100%|██████████| 34/34 [00:03<00:00,  8.61it/s]\n",
      "Train Epoch: [472/500], lr: 0.000232, Loss: 1.5085: 100%|██████████| 34/34 [00:03<00:00,  8.73it/s]\n",
      "Train Epoch: [473/500], lr: 0.000215, Loss: 1.5099: 100%|██████████| 34/34 [00:03<00:00,  8.59it/s]\n",
      "Train Epoch: [474/500], lr: 0.000200, Loss: 1.5106: 100%|██████████| 34/34 [00:03<00:00,  8.71it/s]\n",
      "Train Epoch: [475/500], lr: 0.000185, Loss: 1.5144: 100%|██████████| 34/34 [00:03<00:00,  8.70it/s]\n",
      "Train Epoch: [476/500], lr: 0.000170, Loss: 1.5089: 100%|██████████| 34/34 [00:03<00:00,  8.70it/s]\n",
      "Train Epoch: [477/500], lr: 0.000156, Loss: 1.4924: 100%|██████████| 34/34 [00:03<00:00,  8.74it/s]\n",
      "Train Epoch: [478/500], lr: 0.000143, Loss: 1.5270: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [479/500], lr: 0.000130, Loss: 1.5029: 100%|██████████| 34/34 [00:03<00:00,  8.68it/s]\n",
      "Train Epoch: [480/500], lr: 0.000118, Loss: 1.5115: 100%|██████████| 34/34 [00:03<00:00,  8.69it/s]\n",
      "Train Epoch: [481/500], lr: 0.000107, Loss: 1.4843: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [482/500], lr: 0.000096, Loss: 1.5148: 100%|██████████| 34/34 [00:03<00:00,  8.67it/s]\n",
      "Train Epoch: [483/500], lr: 0.000085, Loss: 1.4944: 100%|██████████| 34/34 [00:03<00:00,  8.63it/s]\n",
      "Train Epoch: [484/500], lr: 0.000076, Loss: 1.4922: 100%|██████████| 34/34 [00:03<00:00,  8.62it/s]\n",
      "Train Epoch: [485/500], lr: 0.000067, Loss: 1.5094: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [486/500], lr: 0.000058, Loss: 1.5273: 100%|██████████| 34/34 [00:03<00:00,  8.64it/s]\n",
      "Train Epoch: [487/500], lr: 0.000050, Loss: 1.4889: 100%|██████████| 34/34 [00:04<00:00,  8.48it/s]\n",
      "Train Epoch: [488/500], lr: 0.000043, Loss: 1.5185: 100%|██████████| 34/34 [00:04<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [489/500], lr: 0.000036, Loss: 1.4766:  91%|█████████ | 31/34 [00:03<00:00, 12.61it/s]"
     ]
    }
   ],
   "source": [
    "# define optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.wd, momentum=0.9)\n",
    "\n",
    "# load model if resume\n",
    "epoch_start = 1\n",
    "if args.resume != '':\n",
    "    checkpoint = torch.load(args.resume)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    epoch_start = checkpoint['epoch'] + 1\n",
    "    print('Loaded from: {}'.format(args.resume))\n",
    "\n",
    "# load previous moco pretrained model\n",
    "#eg: now training on moco Cifar-Luna-Covid, need load cach-moco-Cifar-Luna pretrained model   \n",
    "checkpoint = torch.load('./Covid-cache/cache-moco-Cifar/model_last.pth')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# logging\n",
    "results = {'train_loss': [], 'test_acc@1': []}\n",
    "if not os.path.exists(args.results_dir):\n",
    "    os.mkdir(args.results_dir)\n",
    "# dump args\n",
    "with open(args.results_dir + '/args.json', 'w') as fid:\n",
    "    json.dump(args.__dict__, fid, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epoch_start, args.epochs + 1):\n",
    "    train_loss = train(model, train_loader, optimizer, epoch, args)\n",
    "    results['train_loss'].append(train_loss)\n",
    " #   test_acc_1 = test(model.encoder_q, memory_loader, test_loader, epoch, args)\n",
    "    results['test_acc@1'].append('NaN')\n",
    "    # save statistics\n",
    "    data_frame = pd.DataFrame(data=results, index=range(epoch_start, epoch + 1))\n",
    "    data_frame.to_csv(args.results_dir + '/log.csv', index_label='epoch')\n",
    "    # save model\n",
    "    torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer' : optimizer.state_dict(),}, args.results_dir + '/model_last.pth')\n",
    "    if epoch > 5 and train_loss < min(np.array(results['train_loss'])[3:-1]):\n",
    "        torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer' : optimizer.state_dict(),}, args.results_dir + '/model_best.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Modified from and Referred to: ## \n",
    "\n",
    "[link] (https://colab.research.google.com/github/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb)\n",
    "\n",
    "@Article{he2019moco,\n",
    "  author  = {Kaiming He and Haoqi Fan and Yuxin Wu and Saining Xie and Ross Girshick},\n",
    "  title   = {Momentum Contrast for Unsupervised Visual Representation Learning},\n",
    "  journal = {arXiv preprint arXiv:1911.05722},\n",
    "  year    = {2019},\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "moco_cifar10_demo",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
