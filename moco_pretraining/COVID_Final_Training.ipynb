{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Supervised Learning on Binary COVID Classification Task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import random \n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from skimage.io import imread, imsave\n",
    "import skimage\n",
    "from PIL import ImageFile\n",
    "from PIL import Image\n",
    "import argparse\n",
    "from functools import partial\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Info ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "Tesla V100-SXM2-16GB\n",
      "Wed Apr 20 14:39:56 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:18:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    44W / 300W |      0MiB / 16160MiB |      0%   E. Process |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#show gpu info\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "torch.cuda.current_device()\n",
    "device = 'cuda'\n",
    "\n",
    "gpu_info = !nvidia-smi -i 0\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define all Argument ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(arch='resnet18', batch_size=64, epochs=500, model_load='./Covid-cache/cache-moco-Cifar-Luna-Covid/model_last.pth', results_dir='./Covid-cache/cache-train-Cifar-Luna-Covid', resume=False, start_epoch=1, total_epoch=100)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Final Training Classifier on COVIDCT')\n",
    "\n",
    "parser.add_argument('-a', '--arch', default='resnet18')\n",
    "parser.add_argument('-bs','--batch-size', default=64, type=int, metavar='N', help='mini-batch size')\n",
    "parser.add_argument('--epochs', default=500, type=int, metavar='N', help='number of total epochs to run')\n",
    "parser.add_argument('--results-dir', default='', type=str, metavar='PATH', help='path to cache (default: none)')\n",
    "parser.add_argument('--resume',default=False,help='if resume training ')\n",
    "parser.add_argument('--start-epoch',default = 1, type=int)\n",
    "parser.add_argument('--total-epoch',default = 100, type=int)\n",
    "parser.add_argument('--model-load',default = '',help='pretrained model file path')\n",
    "args = parser.parse_args('')\n",
    "\n",
    "\n",
    "args.results_dir = './Covid-cache/cache-' + 'train-Cifar-Luna-Covid'\n",
    "args.model_load = './Covid-cache/cache-moco-Cifar-Luna-Covid/model_last.pth'\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define metric function \n",
    "\n",
    "def metric(predlist,tarlist,scorelist):\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    for i in range(len(predlist)):\n",
    "        if predlist[i]==1 and tarlist[i]==1:\n",
    "            TP+=1\n",
    "        elif predlist[i]==0 and tarlist[i]==0:\n",
    "            TN+=1\n",
    "        elif predlist[i]==0 and tarlist[i]==1:\n",
    "            FN+=1\n",
    "        elif predlist[i]==1 and tarlist[i]==0:\n",
    "            FP+=1\n",
    "    #TP = ((predlist == 1) & (tarlist == 1)).sum()\n",
    "    #TN = ((predlist == 0) & (tarlist == 0)).sum()\n",
    "    #FN = ((predlist == 0) & (tarlist == 1)).sum()\n",
    "    #FP = ((predlist == 1) & (tarlist == 0)).sum()\n",
    "    \n",
    "    p = TP / (TP + FP)\n",
    "    r = TP / (TP + FN)\n",
    "    F1 = 2 * r * p / (r + p)\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "    AUC = roc_auc_score(tarlist,scorelist)\n",
    "    return TP,TN,FN,FP,p,r,F1,acc,AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Image Augmentation and DataLoader ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define image augmentation for training set and val&test set \n",
    "normalize = transforms.Normalize(mean=[0.45271412, 0.45271412, 0.45271412],\n",
    "                                     std=[0.33165374, 0.33165374, 0.33165374])\n",
    "train_transformer = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.RandomResizedCrop((64),scale=(0.5,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "val_transformer = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425\n",
      "118\n",
      "203\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_txt(txt_path):\n",
    "    with open(txt_path) as f:\n",
    "        lines = f.readlines()\n",
    "    txt_data = [line.strip() for line in lines]\n",
    "    return txt_data\n",
    "\n",
    "class CovidCTDataset(Dataset):\n",
    "    def __init__(self, root_dir, txt_COVID, txt_NonCOVID, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            txt_path (string): Path to the txt file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform: image augmentation \n",
    "        File structure:\n",
    "        - root_dir\n",
    "            - CT_COVID\n",
    "                - img1.png\n",
    "                - img2.png\n",
    "                - ......\n",
    "            - CT_NonCOVID\n",
    "                - img1.png\n",
    "                - img2.png\n",
    "                - ......\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.txt_path = [txt_COVID,txt_NonCOVID]\n",
    "        self.classes = ['CT_COVID', 'CT_NonCOVID']\n",
    "        self.num_cls = len(self.classes)\n",
    "        self.img_list = []\n",
    "        for c in range(self.num_cls):\n",
    "            cls_list = [[os.path.join(self.root_dir,self.classes[c],item), c] for item in read_txt(self.txt_path[c])]\n",
    "            self.img_list += cls_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = self.img_list[idx][0]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        sample = {'img': image,\n",
    "                  'label': int(self.img_list[idx][1])}\n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    trainset = CovidCTDataset(root_dir='/projectnb/dl523/projects/COVIDCT2/dataset/COVIDCT',\n",
    "                              txt_COVID='/projectnb/dl523/projects/COVIDCT2/dataset/COVID-Data-split/COVID/trainCT_COVID.txt',\n",
    "                              txt_NonCOVID='/projectnb/dl523/projects/COVIDCT2/dataset/COVID-Data-split/NonCOVID/trainCT_NonCOVID.txt',\n",
    "                              transform= train_transformer)\n",
    "    valset = CovidCTDataset(root_dir='/projectnb/dl523/projects/COVIDCT2/dataset/COVIDCT',\n",
    "                              txt_COVID='/projectnb/dl523/projects/COVIDCT2/dataset/COVID-Data-split/COVID/valCT_COVID.txt',\n",
    "                              txt_NonCOVID='/projectnb/dl523/projects/COVIDCT2/dataset/COVID-Data-split/NonCOVID/valCT_NonCOVID.txt',\n",
    "                              transform= val_transformer)\n",
    "    testset = CovidCTDataset(root_dir='/projectnb/dl523/projects/COVIDCT2/dataset/COVIDCT',\n",
    "                              txt_COVID='/projectnb/dl523/projects/COVIDCT2/dataset/COVID-Data-split/COVID/testCT_COVID.txt',\n",
    "                              txt_NonCOVID='/projectnb/dl523/projects/COVIDCT2/dataset/COVID-Data-split/NonCOVID/testCT_NonCOVID.txt',\n",
    "                              transform= val_transformer)\n",
    "    print(trainset.__len__())\n",
    "    print(valset.__len__())\n",
    "    print(testset.__len__())\n",
    "\n",
    "    train_loader = DataLoader(trainset, batch_size=args.batch_size,drop_last=True, shuffle=True,num_workers=16)\n",
    "    val_loader = DataLoader(valset, batch_size=args.batch_size, drop_last=False, shuffle=False,num_workers=16)\n",
    "    test_loader = DataLoader(testset, batch_size=args.batch_size,drop_last=False, shuffle=False,num_workers=16)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validation and Test Function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(optimizer, epoch):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    total_num = 0 \n",
    "    \n",
    "    for batch_index, batch_samples in enumerate(train_loader):\n",
    "        \n",
    "        data, target = batch_samples['img'].to(device), batch_samples['label'].to(device) \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        criteria = nn.CrossEntropyLoss()\n",
    "        loss = criteria(output, target.long())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_num += target.size(0)\n",
    "        train_loss += loss.item()*target.size(0)                \n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        train_correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
    "    \n",
    "        if batch_index % 3 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}'.format(\n",
    "                epoch, batch_index, len(train_loader),\n",
    "                100.0 * batch_index / len(train_loader), loss.item()))\n",
    "    return train_loss/total_num      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    total_num = 0 \n",
    "    correct = 0\n",
    "    results = []\n",
    "   \n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        \n",
    "        predlist=[]\n",
    "        scorelist=[]\n",
    "        targetlist=[]\n",
    "        for batch_index, batch_samples in enumerate(val_loader):\n",
    "            data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            total_num += target.size(0)\n",
    "            val_loss += criteria(output, target.long()).item()*target.size(0)\n",
    "            score = F.softmax(output, dim=1)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
    "            targetcpu=target.long().cpu().numpy()\n",
    "            predlist=np.append(predlist, pred.cpu().numpy())\n",
    "            scorelist=np.append(scorelist, score.cpu().numpy()[:,1])\n",
    "            targetlist=np.append(targetlist,targetcpu)\n",
    "           \n",
    "    return val_loss/total_num, targetlist, scorelist, predlist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    total_num = 0 \n",
    "    correct = 0\n",
    "    results = []\n",
    "    \n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        predlist=[]\n",
    "        scorelist=[]\n",
    "        targetlist=[]\n",
    "        for batch_index, batch_samples in enumerate(test_loader):\n",
    "            data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criteria(output, target.long())\n",
    "            \n",
    "            total_num += target.size(0)\n",
    "            test_loss += criteria(output, target.long()).item()/target.size(0)\n",
    "            \n",
    "            score = F.softmax(output, dim=1)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
    "            targetcpu=target.long().cpu().numpy()\n",
    "            predlist=np.append(predlist, pred.cpu().numpy())\n",
    "            scorelist=np.append(scorelist, score.cpu().numpy()[:,1])\n",
    "            targetlist=np.append(targetlist,targetcpu)\n",
    "            \n",
    "    return test_loss/total_num, targetlist, scorelist, predlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet18 Backbone ### \n",
    "\n",
    "**SplitBatchNorm**: simulate the multiple gpu parallel computing in only one GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified ResNet18 model \n",
    "class SplitBatchNorm(nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, num_splits, **kw):\n",
    "        super().__init__(num_features, **kw)\n",
    "        self.num_splits = num_splits\n",
    "        \n",
    "    def forward(self, input):\n",
    "        N, C, H, W = input.shape\n",
    "        if self.training or not self.track_running_stats:\n",
    "            running_mean_split = self.running_mean.repeat(self.num_splits)\n",
    "            running_var_split = self.running_var.repeat(self.num_splits)\n",
    "            outcome = nn.functional.batch_norm(\n",
    "                input.view(-1, C * self.num_splits, H, W), running_mean_split, running_var_split, \n",
    "                self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits),\n",
    "                True, self.momentum, self.eps).view(N, C, H, W)\n",
    "            self.running_mean.data.copy_(running_mean_split.view(self.num_splits, C).mean(dim=0))\n",
    "            self.running_var.data.copy_(running_var_split.view(self.num_splits, C).mean(dim=0))\n",
    "            return outcome\n",
    "        else:\n",
    "            return nn.functional.batch_norm(\n",
    "                input, self.running_mean, self.running_var, \n",
    "                self.weight, self.bias, False, self.momentum, self.eps)\n",
    "\n",
    "\n",
    "class ModelBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Common CIFAR ResNet recipe.\n",
    "    Comparing with ImageNet ResNet recipe, it:\n",
    "    (i) replaces conv1 with kernel=3, str=1\n",
    "    (ii) removes pool1\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=128, arch=None, bn_splits=16):\n",
    "        super(ModelBase, self).__init__()\n",
    "\n",
    "        # use split batchnorm\n",
    "        norm_layer = partial(SplitBatchNorm, num_splits=bn_splits) if bn_splits > 1 else nn.BatchNorm2d\n",
    "        resnet_arch = getattr(resnet, arch)\n",
    "        net = resnet_arch(num_classes=feature_dim, norm_layer=norm_layer)\n",
    "\n",
    "        self.net = []\n",
    "        for name, module in net.named_children():\n",
    "            if name == 'conv1':\n",
    "                module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            if isinstance(module, nn.MaxPool2d):\n",
    "                continue\n",
    "            if isinstance(module, nn.Linear):\n",
    "                self.net.append(nn.Flatten(1))\n",
    "            self.net.append(module)\n",
    "\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        # note: not normalized here\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load The MoCo Pretrained Encoder  and Start Final Training for the Classifier ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelBase(arch='resnet18', bn_splits=8)\n",
    "model.to(device)\n",
    "\n",
    "checkpoint = torch.load(args.model_load)\n",
    "\n",
    "for i in model.state_dict():\n",
    "    if 'encoder_q.'+i in checkpoint['state_dict']:\n",
    "        model.state_dict()[i].copy_(checkpoint['state_dict']['encoder_q.'+i].data)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6 (0%)]\tTrain Loss: 4.836575\n",
      "Train Epoch: 1 [3/6 (50%)]\tTrain Loss: 4.707318\n",
      "Train Epoch: 2 [0/6 (0%)]\tTrain Loss: 4.580256\n",
      "Train Epoch: 2 [3/6 (50%)]\tTrain Loss: 4.486810\n",
      "Train Epoch: 3 [0/6 (0%)]\tTrain Loss: 4.406919\n",
      "Train Epoch: 3 [3/6 (50%)]\tTrain Loss: 4.342517\n",
      "Train Epoch: 4 [0/6 (0%)]\tTrain Loss: 4.271269\n",
      "Train Epoch: 4 [3/6 (50%)]\tTrain Loss: 4.171485\n",
      "Train Epoch: 5 [0/6 (0%)]\tTrain Loss: 4.113281\n",
      "Train Epoch: 5 [3/6 (50%)]\tTrain Loss: 4.066294\n",
      "Train Epoch: 6 [0/6 (0%)]\tTrain Loss: 3.986048\n",
      "Train Epoch: 6 [3/6 (50%)]\tTrain Loss: 3.852019\n",
      "Train Epoch: 7 [0/6 (0%)]\tTrain Loss: 3.898759\n",
      "Train Epoch: 7 [3/6 (50%)]\tTrain Loss: 3.925617\n",
      "Train Epoch: 8 [0/6 (0%)]\tTrain Loss: 3.752017\n",
      "Train Epoch: 8 [3/6 (50%)]\tTrain Loss: 3.667169\n",
      "Train Epoch: 9 [0/6 (0%)]\tTrain Loss: 3.763572\n",
      "Train Epoch: 9 [3/6 (50%)]\tTrain Loss: 3.605447\n",
      "Train Epoch: 10 [0/6 (0%)]\tTrain Loss: 3.544275\n",
      "Train Epoch: 10 [3/6 (50%)]\tTrain Loss: 3.455125\n",
      "\n",
      " The epoch is 10, 10 epoch avg: train loss: 4.0427, val loss: 3.4868, acc: 0.6271, F1: 0.6857, AUC: 0.6325\n",
      "Train Epoch: 11 [0/6 (0%)]\tTrain Loss: 3.568835\n",
      "Train Epoch: 11 [3/6 (50%)]\tTrain Loss: 3.389426\n",
      "Train Epoch: 12 [0/6 (0%)]\tTrain Loss: 3.419889\n",
      "Train Epoch: 12 [3/6 (50%)]\tTrain Loss: 3.124626\n",
      "Train Epoch: 13 [0/6 (0%)]\tTrain Loss: 3.278002\n",
      "Train Epoch: 13 [3/6 (50%)]\tTrain Loss: 3.154845\n",
      "Train Epoch: 14 [0/6 (0%)]\tTrain Loss: 3.164614\n",
      "Train Epoch: 14 [3/6 (50%)]\tTrain Loss: 3.140227\n",
      "Train Epoch: 15 [0/6 (0%)]\tTrain Loss: 2.902998\n",
      "Train Epoch: 15 [3/6 (50%)]\tTrain Loss: 3.011264\n",
      "Train Epoch: 16 [0/6 (0%)]\tTrain Loss: 2.793285\n",
      "Train Epoch: 16 [3/6 (50%)]\tTrain Loss: 2.851590\n",
      "Train Epoch: 17 [0/6 (0%)]\tTrain Loss: 2.697332\n",
      "Train Epoch: 17 [3/6 (50%)]\tTrain Loss: 2.677441\n",
      "Train Epoch: 18 [0/6 (0%)]\tTrain Loss: 2.570586\n",
      "Train Epoch: 18 [3/6 (50%)]\tTrain Loss: 2.527670\n",
      "Train Epoch: 19 [0/6 (0%)]\tTrain Loss: 2.470053\n",
      "Train Epoch: 19 [3/6 (50%)]\tTrain Loss: 2.370804\n",
      "Train Epoch: 20 [0/6 (0%)]\tTrain Loss: 2.360023\n",
      "Train Epoch: 20 [3/6 (50%)]\tTrain Loss: 2.303538\n",
      "\n",
      " The epoch is 20, 10 epoch avg: train loss: 2.8745, val loss: 3.7045, acc: 0.6949, F1: 0.7097, AUC: 0.7928\n",
      "Train Epoch: 21 [0/6 (0%)]\tTrain Loss: 2.245341\n",
      "Train Epoch: 21 [3/6 (50%)]\tTrain Loss: 2.282445\n",
      "Train Epoch: 22 [0/6 (0%)]\tTrain Loss: 2.084325\n",
      "Train Epoch: 22 [3/6 (50%)]\tTrain Loss: 2.135370\n",
      "Train Epoch: 23 [0/6 (0%)]\tTrain Loss: 1.906303\n",
      "Train Epoch: 23 [3/6 (50%)]\tTrain Loss: 1.976929\n",
      "Train Epoch: 24 [0/6 (0%)]\tTrain Loss: 1.828837\n",
      "Train Epoch: 24 [3/6 (50%)]\tTrain Loss: 1.858394\n",
      "Train Epoch: 25 [0/6 (0%)]\tTrain Loss: 1.910871\n",
      "Train Epoch: 25 [3/6 (50%)]\tTrain Loss: 1.848030\n",
      "Train Epoch: 26 [0/6 (0%)]\tTrain Loss: 1.791465\n",
      "Train Epoch: 26 [3/6 (50%)]\tTrain Loss: 1.620399\n",
      "Train Epoch: 27 [0/6 (0%)]\tTrain Loss: 1.445628\n",
      "Train Epoch: 27 [3/6 (50%)]\tTrain Loss: 1.519932\n",
      "Train Epoch: 28 [0/6 (0%)]\tTrain Loss: 1.526339\n",
      "Train Epoch: 28 [3/6 (50%)]\tTrain Loss: 1.402661\n",
      "Train Epoch: 29 [0/6 (0%)]\tTrain Loss: 1.478689\n",
      "Train Epoch: 29 [3/6 (50%)]\tTrain Loss: 1.262164\n",
      "Train Epoch: 30 [0/6 (0%)]\tTrain Loss: 1.337346\n",
      "Train Epoch: 30 [3/6 (50%)]\tTrain Loss: 1.268804\n",
      "\n",
      " The epoch is 30, 10 epoch avg: train loss: 1.7224, val loss: 2.7551, acc: 0.7119, F1: 0.7018, AUC: 0.7911\n",
      "Train Epoch: 31 [0/6 (0%)]\tTrain Loss: 1.138074\n",
      "Train Epoch: 31 [3/6 (50%)]\tTrain Loss: 1.141511\n",
      "Train Epoch: 32 [0/6 (0%)]\tTrain Loss: 1.150898\n",
      "Train Epoch: 32 [3/6 (50%)]\tTrain Loss: 1.081570\n",
      "Train Epoch: 33 [0/6 (0%)]\tTrain Loss: 1.164459\n",
      "Train Epoch: 33 [3/6 (50%)]\tTrain Loss: 0.974702\n",
      "Train Epoch: 34 [0/6 (0%)]\tTrain Loss: 0.910149\n",
      "Train Epoch: 34 [3/6 (50%)]\tTrain Loss: 0.885733\n",
      "Train Epoch: 35 [0/6 (0%)]\tTrain Loss: 0.926881\n",
      "Train Epoch: 35 [3/6 (50%)]\tTrain Loss: 0.748245\n",
      "Train Epoch: 36 [0/6 (0%)]\tTrain Loss: 0.907669\n",
      "Train Epoch: 36 [3/6 (50%)]\tTrain Loss: 0.756313\n",
      "Train Epoch: 37 [0/6 (0%)]\tTrain Loss: 0.828316\n",
      "Train Epoch: 37 [3/6 (50%)]\tTrain Loss: 0.703386\n",
      "Train Epoch: 38 [0/6 (0%)]\tTrain Loss: 0.649948\n",
      "Train Epoch: 38 [3/6 (50%)]\tTrain Loss: 0.680767\n",
      "Train Epoch: 39 [0/6 (0%)]\tTrain Loss: 0.537728\n",
      "Train Epoch: 39 [3/6 (50%)]\tTrain Loss: 0.695811\n",
      "Train Epoch: 40 [0/6 (0%)]\tTrain Loss: 0.657733\n",
      "Train Epoch: 40 [3/6 (50%)]\tTrain Loss: 0.562336\n",
      "\n",
      " The epoch is 40, 10 epoch avg: train loss: 0.8686, val loss: 1.8085, acc: 0.7373, F1: 0.7304, AUC: 0.7882\n",
      "Train Epoch: 41 [0/6 (0%)]\tTrain Loss: 0.630894\n",
      "Train Epoch: 41 [3/6 (50%)]\tTrain Loss: 0.685914\n",
      "Train Epoch: 42 [0/6 (0%)]\tTrain Loss: 0.471698\n",
      "Train Epoch: 42 [3/6 (50%)]\tTrain Loss: 0.508258\n",
      "Train Epoch: 43 [0/6 (0%)]\tTrain Loss: 0.637831\n",
      "Train Epoch: 43 [3/6 (50%)]\tTrain Loss: 0.418290\n",
      "Train Epoch: 44 [0/6 (0%)]\tTrain Loss: 0.473644\n",
      "Train Epoch: 44 [3/6 (50%)]\tTrain Loss: 0.506223\n",
      "Train Epoch: 45 [0/6 (0%)]\tTrain Loss: 0.455475\n",
      "Train Epoch: 45 [3/6 (50%)]\tTrain Loss: 0.602445\n",
      "Train Epoch: 46 [0/6 (0%)]\tTrain Loss: 0.389783\n",
      "Train Epoch: 46 [3/6 (50%)]\tTrain Loss: 0.381848\n",
      "Train Epoch: 47 [0/6 (0%)]\tTrain Loss: 0.465472\n",
      "Train Epoch: 47 [3/6 (50%)]\tTrain Loss: 0.277251\n",
      "Train Epoch: 48 [0/6 (0%)]\tTrain Loss: 0.288439\n",
      "Train Epoch: 48 [3/6 (50%)]\tTrain Loss: 0.411439\n",
      "Train Epoch: 49 [0/6 (0%)]\tTrain Loss: 0.294868\n",
      "Train Epoch: 49 [3/6 (50%)]\tTrain Loss: 0.320239\n",
      "Train Epoch: 50 [0/6 (0%)]\tTrain Loss: 0.335353\n",
      "Train Epoch: 50 [3/6 (50%)]\tTrain Loss: 0.440791\n",
      "\n",
      " The epoch is 50, 10 epoch avg: train loss: 0.4503, val loss: 1.2226, acc: 0.7288, F1: 0.7288, AUC: 0.8138\n",
      "Train Epoch: 51 [0/6 (0%)]\tTrain Loss: 0.385671\n",
      "Train Epoch: 51 [3/6 (50%)]\tTrain Loss: 0.382647\n",
      "Train Epoch: 52 [0/6 (0%)]\tTrain Loss: 0.245278\n",
      "Train Epoch: 52 [3/6 (50%)]\tTrain Loss: 0.256751\n",
      "Train Epoch: 53 [0/6 (0%)]\tTrain Loss: 0.255729\n",
      "Train Epoch: 53 [3/6 (50%)]\tTrain Loss: 0.363088\n",
      "Train Epoch: 54 [0/6 (0%)]\tTrain Loss: 0.321819\n",
      "Train Epoch: 54 [3/6 (50%)]\tTrain Loss: 0.177227\n",
      "Train Epoch: 55 [0/6 (0%)]\tTrain Loss: 0.431550\n",
      "Train Epoch: 55 [3/6 (50%)]\tTrain Loss: 0.307132\n",
      "Train Epoch: 56 [0/6 (0%)]\tTrain Loss: 0.209643\n",
      "Train Epoch: 56 [3/6 (50%)]\tTrain Loss: 0.272597\n",
      "Train Epoch: 57 [0/6 (0%)]\tTrain Loss: 0.201000\n",
      "Train Epoch: 57 [3/6 (50%)]\tTrain Loss: 0.182779\n",
      "Train Epoch: 58 [0/6 (0%)]\tTrain Loss: 0.171331\n",
      "Train Epoch: 58 [3/6 (50%)]\tTrain Loss: 0.404841\n",
      "Train Epoch: 59 [0/6 (0%)]\tTrain Loss: 0.138363\n",
      "Train Epoch: 59 [3/6 (50%)]\tTrain Loss: 0.218084\n",
      "Train Epoch: 60 [0/6 (0%)]\tTrain Loss: 0.250741\n",
      "Train Epoch: 60 [3/6 (50%)]\tTrain Loss: 0.193900\n",
      "\n",
      " The epoch is 60, 10 epoch avg: train loss: 0.2500, val loss: 1.1151, acc: 0.7288, F1: 0.7091, AUC: 0.7954\n",
      "Train Epoch: 61 [0/6 (0%)]\tTrain Loss: 0.175699\n",
      "Train Epoch: 61 [3/6 (50%)]\tTrain Loss: 0.209986\n",
      "Train Epoch: 62 [0/6 (0%)]\tTrain Loss: 0.223452\n",
      "Train Epoch: 62 [3/6 (50%)]\tTrain Loss: 0.183881\n",
      "Train Epoch: 63 [0/6 (0%)]\tTrain Loss: 0.147505\n",
      "Train Epoch: 63 [3/6 (50%)]\tTrain Loss: 0.270129\n",
      "Train Epoch: 64 [0/6 (0%)]\tTrain Loss: 0.134998\n",
      "Train Epoch: 64 [3/6 (50%)]\tTrain Loss: 0.175724\n",
      "Train Epoch: 65 [0/6 (0%)]\tTrain Loss: 0.146527\n",
      "Train Epoch: 65 [3/6 (50%)]\tTrain Loss: 0.163334\n",
      "Train Epoch: 66 [0/6 (0%)]\tTrain Loss: 0.203797\n",
      "Train Epoch: 66 [3/6 (50%)]\tTrain Loss: 0.139345\n",
      "Train Epoch: 67 [0/6 (0%)]\tTrain Loss: 0.168035\n",
      "Train Epoch: 67 [3/6 (50%)]\tTrain Loss: 0.261177\n",
      "Train Epoch: 68 [0/6 (0%)]\tTrain Loss: 0.153330\n",
      "Train Epoch: 68 [3/6 (50%)]\tTrain Loss: 0.144790\n",
      "Train Epoch: 69 [0/6 (0%)]\tTrain Loss: 0.098810\n",
      "Train Epoch: 69 [3/6 (50%)]\tTrain Loss: 0.172573\n",
      "Train Epoch: 70 [0/6 (0%)]\tTrain Loss: 0.177981\n",
      "Train Epoch: 70 [3/6 (50%)]\tTrain Loss: 0.222879\n",
      "\n",
      " The epoch is 70, 10 epoch avg: train loss: 0.1713, val loss: 1.0027, acc: 0.7203, F1: 0.7273, AUC: 0.7974\n",
      "Train Epoch: 71 [0/6 (0%)]\tTrain Loss: 0.108765\n",
      "Train Epoch: 71 [3/6 (50%)]\tTrain Loss: 0.141525\n",
      "Train Epoch: 72 [0/6 (0%)]\tTrain Loss: 0.086252\n",
      "Train Epoch: 72 [3/6 (50%)]\tTrain Loss: 0.156445\n",
      "Train Epoch: 73 [0/6 (0%)]\tTrain Loss: 0.120306\n",
      "Train Epoch: 73 [3/6 (50%)]\tTrain Loss: 0.134266\n",
      "Train Epoch: 74 [0/6 (0%)]\tTrain Loss: 0.315591\n",
      "Train Epoch: 74 [3/6 (50%)]\tTrain Loss: 0.130109\n",
      "Train Epoch: 75 [0/6 (0%)]\tTrain Loss: 0.177283\n",
      "Train Epoch: 75 [3/6 (50%)]\tTrain Loss: 0.144026\n",
      "Train Epoch: 76 [0/6 (0%)]\tTrain Loss: 0.197034\n",
      "Train Epoch: 76 [3/6 (50%)]\tTrain Loss: 0.145079\n",
      "Train Epoch: 77 [0/6 (0%)]\tTrain Loss: 0.099106\n",
      "Train Epoch: 77 [3/6 (50%)]\tTrain Loss: 0.086879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 78 [0/6 (0%)]\tTrain Loss: 0.083952\n",
      "Train Epoch: 78 [3/6 (50%)]\tTrain Loss: 0.068642\n",
      "Train Epoch: 79 [0/6 (0%)]\tTrain Loss: 0.060709\n",
      "Train Epoch: 79 [3/6 (50%)]\tTrain Loss: 0.250042\n",
      "Train Epoch: 80 [0/6 (0%)]\tTrain Loss: 0.129597\n",
      "Train Epoch: 80 [3/6 (50%)]\tTrain Loss: 0.108710\n",
      "\n",
      " The epoch is 80, 10 epoch avg: train loss: 0.1271, val loss: 0.9215, acc: 0.7542, F1: 0.7563, AUC: 0.7968\n",
      "Train Epoch: 81 [0/6 (0%)]\tTrain Loss: 0.058850\n",
      "Train Epoch: 81 [3/6 (50%)]\tTrain Loss: 0.151491\n",
      "Train Epoch: 82 [0/6 (0%)]\tTrain Loss: 0.106053\n",
      "Train Epoch: 82 [3/6 (50%)]\tTrain Loss: 0.081526\n",
      "Train Epoch: 83 [0/6 (0%)]\tTrain Loss: 0.067730\n",
      "Train Epoch: 83 [3/6 (50%)]\tTrain Loss: 0.313310\n",
      "Train Epoch: 84 [0/6 (0%)]\tTrain Loss: 0.094325\n",
      "Train Epoch: 84 [3/6 (50%)]\tTrain Loss: 0.084700\n",
      "Train Epoch: 85 [0/6 (0%)]\tTrain Loss: 0.089086\n",
      "Train Epoch: 85 [3/6 (50%)]\tTrain Loss: 0.058629\n",
      "Train Epoch: 86 [0/6 (0%)]\tTrain Loss: 0.199370\n",
      "Train Epoch: 86 [3/6 (50%)]\tTrain Loss: 0.168682\n",
      "Train Epoch: 87 [0/6 (0%)]\tTrain Loss: 0.103324\n",
      "Train Epoch: 87 [3/6 (50%)]\tTrain Loss: 0.135060\n",
      "Train Epoch: 88 [0/6 (0%)]\tTrain Loss: 0.077150\n",
      "Train Epoch: 88 [3/6 (50%)]\tTrain Loss: 0.062187\n",
      "Train Epoch: 89 [0/6 (0%)]\tTrain Loss: 0.097621\n",
      "Train Epoch: 89 [3/6 (50%)]\tTrain Loss: 0.065042\n",
      "Train Epoch: 90 [0/6 (0%)]\tTrain Loss: 0.055492\n",
      "Train Epoch: 90 [3/6 (50%)]\tTrain Loss: 0.095450\n",
      "\n",
      " The epoch is 90, 10 epoch avg: train loss: 0.0954, val loss: 0.9370, acc: 0.7288, F1: 0.7333, AUC: 0.8069\n",
      "Train Epoch: 91 [0/6 (0%)]\tTrain Loss: 0.077717\n",
      "Train Epoch: 91 [3/6 (50%)]\tTrain Loss: 0.079006\n",
      "Train Epoch: 92 [0/6 (0%)]\tTrain Loss: 0.060362\n",
      "Train Epoch: 92 [3/6 (50%)]\tTrain Loss: 0.059884\n",
      "Train Epoch: 93 [0/6 (0%)]\tTrain Loss: 0.071332\n",
      "Train Epoch: 93 [3/6 (50%)]\tTrain Loss: 0.067570\n",
      "Train Epoch: 94 [0/6 (0%)]\tTrain Loss: 0.071993\n",
      "Train Epoch: 94 [3/6 (50%)]\tTrain Loss: 0.077505\n",
      "Train Epoch: 95 [0/6 (0%)]\tTrain Loss: 0.075250\n",
      "Train Epoch: 95 [3/6 (50%)]\tTrain Loss: 0.051069\n",
      "Train Epoch: 96 [0/6 (0%)]\tTrain Loss: 0.060171\n",
      "Train Epoch: 96 [3/6 (50%)]\tTrain Loss: 0.076300\n",
      "Train Epoch: 97 [0/6 (0%)]\tTrain Loss: 0.048477\n",
      "Train Epoch: 97 [3/6 (50%)]\tTrain Loss: 0.040327\n",
      "Train Epoch: 98 [0/6 (0%)]\tTrain Loss: 0.079461\n",
      "Train Epoch: 98 [3/6 (50%)]\tTrain Loss: 0.065662\n",
      "Train Epoch: 99 [0/6 (0%)]\tTrain Loss: 0.040902\n",
      "Train Epoch: 99 [3/6 (50%)]\tTrain Loss: 0.067928\n",
      "Train Epoch: 100 [0/6 (0%)]\tTrain Loss: 0.058458\n",
      "Train Epoch: 100 [3/6 (50%)]\tTrain Loss: 0.300706\n",
      "\n",
      " The epoch is 100, 10 epoch avg: train loss: 0.0747, val loss: 0.9014, acc: 0.7203, F1: 0.7317, AUC: 0.7954\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "args.resume = False\n",
    "args.start_epoch = 1\n",
    "\n",
    "votenum = 10\n",
    "\n",
    "train_loss = np.zeros(1)\n",
    "val_loss = np.zeros(1)\n",
    "vote_pred = np.zeros(valset.__len__())\n",
    "vote_score = np.zeros(valset.__len__())\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "#for resuming \n",
    "if args.resume == True:\n",
    "    results = pd.read_csv(args.results_dir + '/train_val_log.csv',index_col=0)\n",
    "    checkpoint = torch.load(args.results_dir + '/model_last.pth')\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "else: \n",
    "    results = {'epoch':[],'train_loss':[],'val_loss':[],'val_accurary': [],'val_F1': [],'val_AUC': []}\n",
    "\n",
    "\n",
    "if not os.path.exists(args.results_dir):\n",
    "    os.mkdir(args.results_dir)\n",
    "\n",
    "for epoch in range(args.start_epoch, args.total_epoch+1):\n",
    "    trainloss = train(optimizer, epoch)\n",
    "    \n",
    "    valloss,targetlist, scorelist, predlist = val(epoch)\n",
    "    \n",
    "    train_loss += trainloss\n",
    "    val_loss += valloss \n",
    "    vote_pred += predlist \n",
    "    vote_score += scorelist \n",
    "    \n",
    "    if epoch % votenum == 0:\n",
    "        \n",
    "        # major vote\n",
    "        vote_pred[vote_pred <= (votenum/2)] = 0\n",
    "        vote_pred[vote_pred > (votenum/2)] = 1\n",
    "        vote_score = vote_score/votenum\n",
    "        \n",
    "        TP,TN,FN,FP,p,r,F1,acc,AUC = metric(vote_pred,targetlist,vote_score)\n",
    "        \n",
    "        results['epoch'].append(epoch)\n",
    "        results['train_loss'].append((train_loss/votenum).item())\n",
    "        results['val_loss'].append((val_loss/votenum).item())\n",
    "        results['val_accurary'].append(acc)\n",
    "        results['val_F1'].append(F1)\n",
    "        results['val_AUC'].append(AUC)\n",
    "        data_frame = pd.DataFrame(data=results)\n",
    "        data_frame.to_csv(args.results_dir + '/train_val_log.csv')\n",
    "        \n",
    "        #save the most current model \n",
    "        torch.save({'epoch': epoch, 'state_dict': model.state_dict(),\n",
    "                    'optimizer' : optimizer.state_dict(),}, args.results_dir + '/model_last.pth')\n",
    "        \n",
    "        vote_pred = np.zeros(valset.__len__())\n",
    "        vote_score = np.zeros(valset.__len__())\n",
    "        print('\\n The epoch is {}, 10 epoch avg: train loss: {:.4f}, val loss: {:.4f}, acc: {:.4f}, F1: {:.4f}, AUC: {:.4f}'.format(\n",
    "                epoch,(train_loss/votenum).item(),(val_loss/votenum).item(),acc,F1,AUC))\n",
    "        \n",
    "        train_loss = np.zeros(1)\n",
    "        val_loss = np.zeros(1)\n",
    "        vote_pred = np.zeros(valset.__len__())\n",
    "        vote_score = np.zeros(valset.__len__())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "model = ModelBase(arch='resnet18', bn_splits=8)\n",
    "model.to(device)\n",
    "\n",
    "checkpoint1 = torch.load('Covid-cache/cache-train-random-ini/model_last.pth')\n",
    "checkpoint2 = torch.load('Covid-cache/cache-train-Cifar/model_last.pth')\n",
    "checkpoint3 = torch.load('Covid-cache/cache-train-Cifar-Covid/model_last.pth')\n",
    "checkpoint4 = torch.load('Covid-cache/cache-train-Cifar-Luna-Covid/model_last.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Resnet18 Random Initialization (without Moco Pretraining) ### \n",
    "Final best performance:  acc, F1, AUC score: 0.551, 0.683, 0.715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5517241379310345 0.6829268292682926 0.7146744412050534\n"
     ]
    }
   ],
   "source": [
    "#Resnet18 Random Initialization \n",
    "args.batch_size = 1\n",
    "\n",
    "model.load_state_dict(checkpoint1['state_dict'])\n",
    "epoch = 1\n",
    "\n",
    "vote_pred = np.zeros(testset.__len__())\n",
    "vote_score = np.zeros(testset.__len__())\n",
    "\n",
    "\n",
    "testloss,targetlist, scorelist, predlist = test(epoch)\n",
    "#print('target',targetlist)\n",
    "#print('score',scorelist)\n",
    "#print('predict',predlist)\n",
    "vote_pred = vote_pred + predlist \n",
    "vote_score = vote_score + scorelist \n",
    "\n",
    "TP,TN,FN,FP,p,r,F1,acc,AUC = metric(vote_pred,targetlist,vote_score)\n",
    "print(acc,F1,AUC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. one round of MoCo pretraining on Cifar-10 Dataset  ### \n",
    "Final best performance:  acc, F1, AUC score: 0.695, 0.750, 0.818"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6945812807881774 0.7499999999999999 0.8179786200194364\n"
     ]
    }
   ],
   "source": [
    "#moco: Resnet18 pretrained on Cifar \n",
    "args.batch_size = 1\n",
    "\n",
    "model.load_state_dict(checkpoint2['state_dict'])\n",
    "epoch = 1\n",
    "\n",
    "vote_pred = np.zeros(testset.__len__())\n",
    "vote_score = np.zeros(testset.__len__())\n",
    "\n",
    "\n",
    "testloss,targetlist, scorelist, predlist = test(epoch)\n",
    "#print('target',targetlist)\n",
    "#print('score',scorelist)\n",
    "#print('predict',predlist)\n",
    "vote_pred = vote_pred + predlist \n",
    "vote_score = vote_score + scorelist \n",
    "\n",
    "TP,TN,FN,FP,p,r,F1,acc,AUC = metric(vote_pred,targetlist,vote_score)\n",
    "print(acc,F1,AUC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. two rounds of MoCo pretraining on Cifar-10 and COVID-CT unlabeled Dataset  ### \n",
    "Final best performance:  acc, F1, AUC score: 0.783, 0.796, 0.880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7832512315270936 0.7962962962962963 0.8798833819241981\n"
     ]
    }
   ],
   "source": [
    "#moco: Resnet18 pretrained on Cifar and COVIDCT\n",
    "args.batch_size = 1\n",
    "\n",
    "model.load_state_dict(checkpoint3['state_dict'])\n",
    "epoch = 1\n",
    "\n",
    "vote_pred = np.zeros(testset.__len__())\n",
    "vote_score = np.zeros(testset.__len__())\n",
    "\n",
    "\n",
    "testloss,targetlist, scorelist, predlist = test(epoch)\n",
    "#print('target',targetlist)\n",
    "#print('score',scorelist)\n",
    "#print('predict',predlist)\n",
    "vote_pred = vote_pred + predlist \n",
    "vote_score = vote_score + scorelist \n",
    "\n",
    "TP,TN,FN,FP,p,r,F1,acc,AUC = metric(vote_pred,targetlist,vote_score)\n",
    "print(acc,F1,AUC)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. three rounds of MoCo pretraining on Cifar-10, LUNA-CT and COVID-CT unlabeled Dataset  ### \n",
    "Final best performance:  acc, F1, AUC score: 0.695, 0.677, 0.770"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6945812807881774 0.6770833333333333 0.7699708454810495\n"
     ]
    }
   ],
   "source": [
    "#moco: Resnet18 pretrained on Cifar, Luna and COVIDCT\n",
    "args.batch_size = 10\n",
    "\n",
    "model.load_state_dict(checkpoint4['state_dict'])\n",
    "epoch = 1\n",
    "\n",
    "vote_pred = np.zeros(testset.__len__())\n",
    "vote_score = np.zeros(testset.__len__())\n",
    "\n",
    "\n",
    "testloss,targetlist, scorelist, predlist = test(epoch)\n",
    "#print('target',targetlist)\n",
    "#print('score',scorelist)\n",
    "#print('predict',predlist)\n",
    "vote_pred = vote_pred + predlist \n",
    "vote_score = vote_score + scorelist \n",
    "\n",
    "TP,TN,FN,FP,p,r,F1,acc,AUC = metric(vote_pred,targetlist,vote_score)\n",
    "print(acc,F1,AUC)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Code Refer to: ### \n",
    "[link] (https://github.com/UCSD-AI4H/COVID-CT/blob/master/baseline%20methods/Self-Trans/CT-predict-pretrain.ipynb)\n",
    "\n",
    "\n",
    "@article{zhao2020COVID-CT-Dataset,\n",
    "  title={COVID-CT-Dataset: a CT scan dataset about COVID-19},\n",
    "  author={Zhao, Jinyu and Zhang, Yichen and He, Xuehai and Xie, Pengtao},\n",
    "  journal={arXiv preprint arXiv:2003.13865}, \n",
    "  year={2020}\n",
    "}\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
